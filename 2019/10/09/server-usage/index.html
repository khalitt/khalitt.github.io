<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>服务器Slurm的使用 | null</title><meta name="keywords" content="server,submitit"><meta name="author" content="Walter"><meta name="copyright" content="Walter"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Slurm服务器的常用概念Quick Start User Guide  --ntasks 对应 进程（Process），即设定脚本在几个并行任务中 同时执行。可以进一步更改--ntasks-per-node来设定每个node的进程数。默认是一个node的每一个task一个进程  --cpus-per-task （-c）实际设定的是 线程(Thread)数  task：即 process ，进程，">
<meta property="og:type" content="article">
<meta property="og:title" content="服务器Slurm的使用">
<meta property="og:url" content="https://khalitt.github.io/2019/10/09/server-usage/index.html">
<meta property="og:site_name">
<meta property="og:description" content="Slurm服务器的常用概念Quick Start User Guide  --ntasks 对应 进程（Process），即设定脚本在几个并行任务中 同时执行。可以进一步更改--ntasks-per-node来设定每个node的进程数。默认是一个node的每一个task一个进程  --cpus-per-task （-c）实际设定的是 线程(Thread)数  task：即 process ，进程，">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2019-10-09T03:06:35.000Z">
<meta property="article:modified_time" content="2021-03-12T01:11:34.905Z">
<meta property="article:author" content="Walter">
<meta property="article:tag" content="server">
<meta property="article:tag" content="submitit">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://khalitt.github.io/2019/10/09/server-usage/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"5100G1WE6M","apiKey":"c2a0f74ed768aa33e0c961937aa244bc","indexName":"prod_khalitt_github_io","hits":{"per_page":8},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '服务器Slurm的使用',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-03-12 09:11:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.3.0"><link rel="alternate" href="/atom.xml" title="null" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">55</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">20</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Music"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/Movie"><i class="fa-fw /movies/"></i><span> 1</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Music"><i class="fa-fw /music/"></i><span> 0</span></a></li><li><a class="site-page child" href="/Movie"><i class="fa-fw /movies/"></i><span> 1</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">服务器Slurm的使用</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-10-09T03:06:35.000Z" title="Created 2019-10-09 11:06:35">2019-10-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-03-12T01:11:34.905Z" title="Updated 2021-03-12 09:11:34">2021-03-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/server/">server</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">9.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>41min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="服务器Slurm的使用"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Slurm服务器的常用概念"><a href="#Slurm服务器的常用概念" class="headerlink" title="Slurm服务器的常用概念"></a>Slurm服务器的常用概念</h1><p><a target="_blank" rel="noopener" href="https://slurm.schedmd.com/quickstart.html">Quick Start User Guide</a></p>
<ul>
<li><p><code>--ntasks</code> 对应 <strong>进程（Process）</strong>，即设定脚本在几个并行任务中 <strong>同时执行</strong>。可以进一步更改<code>--ntasks-per-node</code>来设定每个node的进程数。<strong>默认是一个node的每一个task一个进程</strong></p>
</li>
<li><p><code>--cpus-per-task</code> （<code>-c</code>）实际设定的是 <strong>线程(Thread)数</strong></p>
</li>
<li><p><strong>task</strong>：即 <strong>process</strong> ，进程，通过设定  <code>--ntasks</code> 即设定了pytorch中的 world size</p>
<blockquote>
<p>tasks：任务数，单个作业(job)或作业步(job step)可有多个任务，一般一个任务需一个CPU核，<br>可理解为所需的CPU核数。 </p>
</blockquote>
</li>
<li><p>job: 即在一段时间内分配给一个用户的资源。一条srun指令或者一条salloc + shell 文件 或者一个sbatch指令就是 <strong>一个job</strong></p>
</li>
<li><p>job steps：每一个job中可以有子命令，即step。最直观的例子，就是多线程应用，在shell文件中有 <strong>多条srun语句， 则每一条srun语句都对应一个step</strong>。</p>
<p>下面的例子中，可以看到<code>-n</code>指定tasks对sbatch并不起效（只运行了一次，并没有运行两次），虽然 <strong>给主joob分配了两个CPU</strong>，因此下面的脚本才能得以运行</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ <span class="token function">cat</span> test_multiprocess.sh
<span class="token comment">#!/bin/sh</span>
<span class="token builtin class-name">echo</span> <span class="token environment constant">JOB</span> <span class="token variable">$SLURM_JOB_ID</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token variable">$CUDA_VISIBLE_DEVICES</span>
srun --gres<span class="token operator">=</span>gpu:2 -n1 --exclusive ./show_device.sh <span class="token operator">&amp;</span>
srun --gres<span class="token operator">=</span>gpu:1 -n1 --exclusive ./show_device.sh <span class="token operator">&amp;</span>
<span class="token function">wait</span>
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ sbatch --gres<span class="token operator">=</span>gpu:3 -n2 test_multiprocess.sh
Submitted batch job <span class="token number">9442</span>
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ <span class="token function">cat</span> slurm-9442.out
<span class="token environment constant">JOB</span> <span class="token number">9442</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">3,4</span>,6
<span class="token environment constant">JOB</span> <span class="token number">9442</span> STEP <span class="token number">0</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">3</span>
<span class="token environment constant">JOB</span> <span class="token number">9442</span> STEP <span class="token number">1</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">4,6</span>
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ sacct -j <span class="token number">9442</span>
       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode
------------ ---------- ---------- ---------- ---------- ---------- --------
<span class="token number">9442</span>         test_mult+       defq      <span class="token function">users</span>          <span class="token number">2</span>  COMPLETED      <span class="token number">0</span>:0
<span class="token number">9442</span>.batch        batch                 <span class="token function">users</span>          <span class="token number">2</span>  COMPLETED      <span class="token number">0</span>:0
<span class="token number">9442.0</span>       show_devi+                 <span class="token function">users</span>          <span class="token number">1</span>  COMPLETED      <span class="token number">0</span>:0
<span class="token number">9442.1</span>       show_devi+                 <span class="token function">users</span>          <span class="token number">1</span>  COMPLETED      <span class="token number">0</span>:0
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>若在sbatch时 <strong>不显式地分配两个task，会报错</strong></p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ sbatch --gres<span class="token operator">=</span>gpu:3 test_multiprocess.sh
Submitted batch job <span class="token number">9443</span>
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ sacct -j <span class="token number">9443</span>
       JobID    JobName  Partition    Account  AllocCPUS      State ExitCode
------------ ---------- ---------- ---------- ---------- ---------- --------
<span class="token number">9443</span>         test_mult+       defq      <span class="token function">users</span>          <span class="token number">1</span>  COMPLETED      <span class="token number">0</span>:0
<span class="token number">9443</span>.batch        batch                 <span class="token function">users</span>          <span class="token number">1</span>  COMPLETED      <span class="token number">0</span>:0
<span class="token number">9443.0</span>       show_devi+                 <span class="token function">users</span>          <span class="token number">1</span>  COMPLETED      <span class="token number">0</span>:0
<span class="token number">9443.1</span>       show_devi+                 <span class="token function">users</span>          <span class="token number">1</span>  COMPLETED      <span class="token number">0</span>:0
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ <span class="token function">cat</span> slurm-9443.out
<span class="token environment constant">JOB</span> <span class="token number">9443</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">3,4</span>,6
<span class="token environment constant">JOB</span> <span class="token number">9443</span> STEP <span class="token number">0</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">3</span>
srun: Job <span class="token number">9443</span> step creation temporarily disabled, retrying
srun: Step created <span class="token keyword">for</span> job <span class="token number">9443</span>
<span class="token environment constant">JOB</span> <span class="token number">9443</span> STEP <span class="token number">1</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">3,4</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



</li>
</ul>
<h2 id="20200203-update"><a href="#20200203-update" class="headerlink" title="20200203 update"></a>20200203 update</h2><p>参考：</p>
<ol>
<li>依旧是先看 quick start <a target="_blank" rel="noopener" href="https://slurm.schedmd.com/quickstart.html">Quick Start User Guide</a></li>
<li><a target="_blank" rel="noopener" href="https://slurm.schedmd.com/job_launch.html">Job Launch Design Guide</a>一个对整个流程简单的描述，可以看看</li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/46532581">How do the terms “job”, “task”, and “step” relate to each other?</a>，推荐！举了一个简单的例子来说明</li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/52438129">How to submit parallel job steps with SLURM?</a>，一个有问题的例子，也不错</li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/53759961">What does the –ntasks or -n tasks does in SLURM?</a>，解释了<code>ntasks</code>的含义，虽然没有直接说明task到底是什么，但是可以大概理解</li>
</ol>
<p>首先根据quick start</p>
<blockquote>
<p><strong>nodes</strong>, the compute resource in Slurm, </p>
<p><strong>partitions</strong>, which group nodes into logical (possibly overlapping) sets </p>
<p><strong>jobs</strong>, or allocations of resources assigned to a user for a specified amount of time</p>
<p><strong>job steps</strong>, which are <strong>sets of (possibly parallel) tasks</strong> within a job</p>
<p>The <strong>partitions</strong> can be considered job queues, each of which has <strong>an assortment of constraints</strong> such as job size limit, job time limit, users permitted to use it, etc. Priority-ordered jobs are allocated nodes within a partition until the resources (nodes, processors, memory, etc.) within that partition are exhausted. </p>
<p>Once a job is assigned a set of nodes, the user is able to initiate parallel work in the form of job steps in any configuration within the allocation.</p>
<p>For instance, <strong>a single job step may be started</strong> that <strong>utilizes all nodes allocated to the job</strong>, <strong>or several job steps</strong> may <strong>independently use a portion of the allocation.</strong></p>
<p><img src="/2019/10/09/server-usage/image-20210203120440380.png" alt="image-20210203120440380"></p>
</blockquote>
<p>可以看到在quick start里面是<strong>没有对task的描述</strong></p>
<p>根据参考2，大概是这样的关系：job &gt; job steps &gt; tasks</p>
<blockquote>
<p><strong>A <em>job</em> consists in one or more <em>steps</em>, each consisting in one or more <em>tasks</em> each using one or more <em>CPU</em>.</strong></p>
<p>Jobs are typically created with the <code>sbatch</code> command, steps are created with the <code>srun</code> command, tasks are requested, at the job level with <code>--ntasks</code> or <code>--ntasks-per-node</code>, or at the step level with <code>--ntasks</code>. CPUs are requested <strong>per task</strong> with <code>--cpus-per-task</code>. Note that jobs submitted with <code>sbatch</code> have one implicit step; the Bash script itself.</p>
<p>Assume the hypothetical job:</p>
<pre class="line-numbers language-none"><code class="language-none">#SBATCH --nodes 8
#SBATCH --tasks-per-node 8
# The job requests 64 CPUs, on 8 nodes.    

# First step, with a sub-allocation of 8 tasks (one per node) to create a tmp dir. 
# No need for more than one task per node, but it has to run on every node
srun --nodes 8 --ntasks 8 mkdir -p &#x2F;tmp&#x2F;$USER&#x2F;$SLURM_JOBID

# Second step with the full allocation (64 tasks) to run an MPI 
# program on some data to produce some output.
srun process.mpi &lt;input.dat &gt;output.txt

# Third step with a sub allocation of 48 tasks (because for instance 
# that program does not scale as well) to post-process the output and 
# extract meaningful information
srun --ntasks 48 --nodes 6 --exclusive postprocess.mpi &lt;output.txt &gt;result.txt &amp;

# Four step with a sub-allocation on a single node (because maybe 
# it is a multithreaded program that cannot use CPUs on distinct nodes)    
# to compress the raw output. This step runs at the same time as 
# the previous one thanks to the ampersand &#96;&amp;&#96; 
OMP_NUM_THREAD&#x3D;12 srun --ntasks 12 --nodes 1 --exclusive compress output.txt &amp;

wait<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p><strong>Four steps were created</strong> and so the accounting information for that job will have 5 lines; <strong>one per step plus one for the Bash script itself</strong>.</p>
</blockquote>
<p>参考5中的说法：</p>
<blockquote>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token comment">#SBATCH --ntasks=2</span>

srun --ntasks<span class="token operator">=</span><span class="token number">1</span> <span class="token function">sleep</span> <span class="token number">10</span> <span class="token operator">&amp;</span> 
srun --ntasks<span class="token operator">=</span><span class="token number">1</span> <span class="token function">sleep</span> <span class="token number">12</span> <span class="token operator">&amp;</span>
<span class="token function">wait</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<p><strong>Each task inherits the parameters specified for the batch script</strong>. This is why <code>--ntasks=1</code> needs to be specified for each srun task, otherwise <strong>each task uses <code>--ntasks=2</code> and so the second command will not run until the first task has finished.</strong></p>
<p>Another caveat of the tasks inheriting the batch parameters is if <code>--export=NONE</code> is specified as a batch parameter. In this case <code>--export=ALL</code> should be specified for each srun command otherwise environment variables set within the sbatch script are not inherited by the srun command.</p>
<p>Additional notes:<br>When using bash pipes, <strong>it may be necessary to specify –nodes=1 to prevent commands either side of the pipes running on separate nodes.</strong><br><strong>When using <code>&amp;</code> to run commands simultaneously, the <code>wait</code> is vital.</strong> In this case, without the <code>wait</code> command, task 0 would cancel itself, given task 1 completed successfully.</p>
</blockquote>
<p>结合上面这些例子，大概可以理解为：</p>
<ul>
<li>task：我们真正需要解决的<strong>“任务”</strong>，即每次执行的最细粒度，在并行计算中，显然就是最小的一次执行。默认情况（<code>-c, --cpus-per-task = 1</code>的情况）下，一个task就是一个cpu</li>
<li>job step：可以简单地理解为每个<code>salloc</code>所运行的或者<code>sbatch</code>脚本中的一条<code>srun</code>就是一个<code>job step</code>了，因为一条<code>srun</code>就已经会对应一个<code>jobid</code>，而每个<code>job step</code>也即每条<code>srun</code>指令都可以执行多个task，通过<code>-n, --ntasks</code>指定每个<code>job</code>要执行的task个数，在默认情况（<code>-c, --cpus-per-task = 1</code>的情况）下，<strong>即对应每个<code>job</code>所需的<code>cpu</code>数</strong></li>
<li>job：可以理解为一个<code>salloc</code>任务 或者 一个<code>sbatch</code>脚本 或者 直接执行的一条<code>srun</code>指令（比如<code>srun --pty bash</code>使用交互式的bash），每个job都可以包含多个job step(即嵌套的<code>srun</code>)</li>
</ul>
<h1 id="Slurm-服务器使用常用命令"><a href="#Slurm-服务器使用常用命令" class="headerlink" title="Slurm 服务器使用常用命令"></a>Slurm 服务器使用常用命令</h1><h2 id="支持interactive-mode的bash：–pty"><a href="#支持interactive-mode的bash：–pty" class="headerlink" title="支持interactive mode的bash：–pty"></a>支持interactive mode的bash：–pty</h2><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">srun --gres<span class="token operator">=</span>gpu:2 --pty python xxx.py<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>这样的话，python中的ipdb等模块就能正常起效了，python自身的补全等功能也实现</p>
<h2 id="salloc-srun-sbatch区别"><a href="#salloc-srun-sbatch区别" class="headerlink" title="salloc srun sbatch区别"></a>salloc srun sbatch区别</h2><p><a target="_blank" rel="noopener" href="https://slurm.schedmd.com/faq.html#sbatch_srun"><strong>What is the difference between the sbatch and srun commands?</strong></a></p>
<ul>
<li><code>salloc</code>只分配资源，<code>srun</code>先分配资源并且执行命令(如果不是实现确定好的话)</li>
<li>srun主要是为了interactive use，sbatch则是批量执行，并且支持job arrays</li>
</ul>
<p>直接参考man解释：</p>
<blockquote>
<p>srun：<strong>Run a parallel job</strong> on cluster managed by Slurm. If necessary, srun <strong>will first create a resource allocation</strong> in which to run the parallel job.</p>
<p>salloc：<strong>Obtain a Slurm job allocation</strong> (a set of nodes), <strong>execute a command</strong>, and then release the allocation when the command is finished.</p>
</blockquote>
<p>参考 <a target="_blank" rel="noopener" href="https://support.nesi.org.nz/hc/en-gb/articles/360001316356-Slurm-Interactive-Sessions">Slurm Interactive Sessions</a></p>
<blockquote>
<p><code>srun</code> will add your resource request to the queue. When the allocation starts, a new bash session will start up on <strong>one of the granted nodes.</strong></p>
<p><code>salloc</code> functions similarly <code>srun --pty bash</code> in that it will add your resource request to the queue. However the allocation starts, a new bash session will start up on <strong>the login node.</strong> This is useful for running a GUI on the login node, but your processes on the compute nodes.</p>
</blockquote>
<p>显然如果是想在管理节点上有一些输入，但是把计算放在计算节点，这个就很有用。注意salloc只是预先划分资源，<strong>并不一定马上执行你真正要执行的计算命令</strong></p>
<h1 id="嵌套srun运行并不可行"><a href="#嵌套srun运行并不可行" class="headerlink" title="嵌套srun运行并不可行"></a>嵌套srun运行并不可行</h1><p>直接嵌套会导致 <strong>子任务（job step）</strong> 无法运行。</p>
<p>正确的做法是把srun写在<code>shell</code>脚本中，用<code>salloc</code>来运行，或者直接运行bash脚本（如果里面没有多线程的要求的话）</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ <span class="token function">cat</span> test2.bash
<span class="token comment">#!/bin/bash</span>
<span class="token comment"># echo JOB $SLURM_JOB_ID CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES</span>
<span class="token builtin class-name">echo</span> <span class="token environment constant">JOB</span> <span class="token variable">$SLURM_JOB_ID</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token variable">$CUDA_VISIBLE_DEVICES</span>
srun --gres<span class="token operator">=</span>gpu:2 -n1 --exclusive ./show_device.sh <span class="token operator">&amp;</span>
<span class="token comment">#METHOD=cpu_forward srun --gres=gpu:1 -n1 --exclusive ./show_device.sh &amp;</span>
srun --gres<span class="token operator">=</span>gpu:1 -n1 --exclusive ./show_device.sh <span class="token operator">&amp;</span>
<span class="token comment">#srun --gres=gpu:1 -n1 --exclusive ./show_device.sh &amp;</span>
<span class="token function">wait</span>
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ salloc --gres<span class="token operator">=</span>gpu:3 -n2 ./test2.bash
salloc: Granted job allocation <span class="token number">9430</span>
<span class="token environment constant">JOB</span> <span class="token number">9430</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span>
srun: Step created <span class="token keyword">for</span> job <span class="token number">9430</span>
srun: Step created <span class="token keyword">for</span> job <span class="token number">9430</span>
<span class="token environment constant">JOB</span> <span class="token number">9430</span> STEP <span class="token number">0</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">3,4</span>
<span class="token environment constant">JOB</span> <span class="token number">9430</span> STEP <span class="token number">1</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">6</span>
salloc: Relinquishing job allocation <span class="token number">9430</span>
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>可以看到上面的例子中，由于 <strong>运行该bash文件时并没有用srun，而是直接运行了，因此此时对于主任务来说是没有available GPU的</strong></p>
<h1 id="srun（salloc）的常用arguments"><a href="#srun（salloc）的常用arguments" class="headerlink" title="srun（salloc）的常用arguments"></a>srun（salloc）的常用arguments</h1><h2 id="n-–ntasks-：运行-lt-number-gt-个任务-运行-lt-number-gt-个任务"><a href="#n-–ntasks-：运行-lt-number-gt-个任务-运行-lt-number-gt-个任务" class="headerlink" title="-n, –ntasks=：运行&lt;number&gt;个任务(运行&lt;number&gt;个任务)"></a>-n, –ntasks=：运行&lt;number&gt;个任务(运行&lt;number&gt;个任务)</h2><blockquote>
<p><strong>-n</strong>, <strong>–ntasks</strong>=&lt;*number*&gt;</p>
<p>Specify the number of tasks to run. Request that <strong>srun</strong> allocate resources for <em>ntasks</em> tasks. The default is one task per node, but note that the <strong>–cpus-per-task</strong> option will change this default. This option applies to job and step allocations.</p>
</blockquote>
<p>-n, –ntasks=&lt;number&gt;：运行&lt;number&gt;个任务，默认一个节点一个作业(job)，<strong>注意是所需总CPU核数。仅对作业起作用，不对作业步(job steps)起作用。即把对应的执行的指令运行n次</strong></p>
<h2 id="SLURM-PROCID-：可理解为子进程的相对ID-SLURM-TASK-PID-则是在系统中实际的PID"><a href="#SLURM-PROCID-：可理解为子进程的相对ID-SLURM-TASK-PID-则是在系统中实际的PID" class="headerlink" title="SLURM_PROCID ：可理解为子进程的相对ID SLURM_TASK_PID 则是在系统中实际的PID"></a><code>SLURM_PROCID</code> ：可理解为子进程的相对ID <code>SLURM_TASK_PID</code> 则是在系统中实际的PID</h2><blockquote>
<p><strong>SLURM_PROCID</strong>：</p>
<p>The MPI rank (or relative process ID) of the current process.</p>
</blockquote>
<p>即并行运算时，各个<strong>线程</strong>独有的PID，一般从0开始</p>
<blockquote>
<p><strong>SLURM_TASK_PID</strong></p>
<p>The process ID of the task being started.</p>
</blockquote>
<p>则是每个子进程在系统的实际的PID</p>
<h2 id="gres-lt-list-gt-是针对每个node来讲的"><a href="#gres-lt-list-gt-是针对每个node来讲的" class="headerlink" title="gres=&lt;list&gt; 是针对每个node来讲的"></a><code>gres=&lt;list&gt;</code> 是针对每个node来讲的</h2><p>来自srun的documentation：</p>
<blockquote>
<p>The specified resources will be allocated to the job <strong>on each node</strong>. </p>
</blockquote>
<p>因此，下面的命令等价于在每个node上都分配两个GPU，然后即总共申请了 <strong>四个GPU</strong></p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">srun --gres<span class="token operator">=</span>gpu:2 -N2<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<p>下面的例子可以看到，申请了两个node之后，会默认把Process数变成2，同时把两个程序同时在两个node中运行（并行计算），并且每个node有<code>gres</code>数的gpu：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm distributed_training<span class="token punctuation">]</span>$ <span class="token function">cat</span> view_multinode.py
<span class="token function">import</span> os
print<span class="token punctuation">(</span><span class="token string">'SLURM_NTASKS:&#123;&#125;'</span>.format<span class="token punctuation">(</span>os.environ<span class="token punctuation">[</span><span class="token string">'SLURM_NTASKS'</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
print<span class="token punctuation">(</span><span class="token string">'SLURM_NODEID:&#123;&#125;'</span>.format<span class="token punctuation">(</span>os.environ<span class="token punctuation">[</span><span class="token string">'SLURM_NODEID'</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
print<span class="token punctuation">(</span><span class="token string">'SLURM_PROCID:&#123;&#125;'</span>.format<span class="token punctuation">(</span>os.environ<span class="token punctuation">[</span><span class="token string">'SLURM_PROCID'</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
print<span class="token punctuation">(</span><span class="token string">'SLURM_LOCALID:&#123;&#125;'</span>.format<span class="token punctuation">(</span>os.environ<span class="token punctuation">[</span><span class="token string">'SLURM_LOCALID'</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
print<span class="token punctuation">(</span><span class="token string">'SLURM_STEP_NODELIST:&#123;&#125;'</span>.format<span class="token punctuation">(</span>os.environ<span class="token punctuation">[</span><span class="token string">'SLURM_STEP_NODELIST'</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
print<span class="token punctuation">(</span><span class="token string">'CUDA_VISIBLE_DEVICES:&#123;&#125;'</span>.format<span class="token punctuation">(</span>os.environ<span class="token punctuation">[</span><span class="token string">'CUDA_VISIBLE_DEVICES'</span><span class="token punctuation">]</span><span class="token punctuation">))</span>
print<span class="token punctuation">(</span><span class="token string">'='</span>*70<span class="token punctuation">)</span>
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm distributed_training<span class="token punctuation">]</span>$ srun --gres<span class="token operator">=</span>gpu:2 -N2 python view_multinode.py
SLURM_NTASKS:2
SLURM_NODEID:1
SLURM_PROCID:1
SLURM_LOCALID:0
SLURM_STEP_NODELIST:node<span class="token punctuation">[</span>01-02<span class="token punctuation">]</span>
CUDA_VISIBLE_DEVICES:0,3
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
SLURM_NTASKS:2
SLURM_NODEID:0
SLURM_PROCID:0
SLURM_LOCALID:0
SLURM_STEP_NODELIST:node<span class="token punctuation">[</span>01-02<span class="token punctuation">]</span>
CUDA_VISIBLE_DEVICES:0,1
<span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span><span class="token operator">==</span>
<span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm distributed_training<span class="token punctuation">]</span>$
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h2 id="–exclusive-常用于保证每个job-step都能够独占资源，不会互相影响"><a href="#–exclusive-常用于保证每个job-step都能够独占资源，不会互相影响" class="headerlink" title="–exclusive 常用于保证每个job step都能够独占资源，不会互相影响"></a>–exclusive 常用于保证每个job step都能够独占资源，不会互相影响</h2><blockquote>
<p><strong>–exclusive[=user|mcs]</strong></p>
<p><strong>This option applies to job and job step allocations</strong>, and has two slightly different meanings for each one. When used to initiate a job, the job allocation cannot share nodes with other running jobs (or just other users with the “=user” option or “=mcs” option). The default shared/exclusive behavior depends on system configuration and the partition’s <strong>OverSubscribe</strong> option takes precedence over the job’s option.</p>
<p>This option can also be used <strong>when initiating more than one job step within an existing resource allocation</strong> (default), where <strong>you want separate processors to be dedicated to each job step</strong>. If sufficient processors are not available to initiate the job step, <strong>it will be deferred.</strong> This can be thought of as providing a mechanism for resource management to the job within its allocation (<strong>–exact</strong> implied).</p>
<p><strong>The exclusive allocation of CPUs applies to job steps by default</strong>. In order to share the resources use the <strong>–overlap</strong> option.</p>
</blockquote>
<p>通过说明可知，默认情况下<code>cpu</code>是不会共享的。但是gpu即<code>gres</code>则会共享，因此如果想要不同job step之间不共享<code>gpu</code>，则需要指定<code>--exclusive</code></p>
<h2 id="c-–cpus-per-task-每个task能够获取到的cpu数。对应于num-workers"><a href="#c-–cpus-per-task-每个task能够获取到的cpu数。对应于num-workers" class="headerlink" title="-c, –cpus-per-task 每个task能够获取到的cpu数。对应于num_workers"></a>-c, –cpus-per-task 每个task能够获取到的cpu数。对应于<code>num_workers</code></h2><blockquote>
<p><strong>-c</strong>, <strong>–cpus-per-task</strong>=&lt;*ncpus*&gt;</p>
<p>Request that <strong><em>ncpus</em> be allocated per process**</strong>. This may be useful if the job is multithreaded and requires more than one CPU per task for optimal performance. The default is one CPU per process. If <strong>-c</strong> is specified without <strong>-n</strong>, as many tasks will be allocated per node as possible while satisfying the <strong>-c</strong> restriction. For instance on a cluster with 8 CPUs per node, a job request for 4 nodes and 3 CPUs per task may be allocated 3 or 6 CPUs per node (1 or 2 tasks per node) depending upon resource consumption by other jobs. Such a job may be unable to execute more than a total of 4 tasks.</p>
<p><strong>WARNING</strong>: There are configurations and options interpreted differently by job and job step requests which can result in inconsistencies for this option. For example <em>srun -c2 –threads-per-core=1 prog</em> may allocate two cores for the job, but if each of those cores contains two threads, the job allocation will include four CPUs. The job step allocation will then launch two threads per CPU for a total of two tasks.</p>
<p><strong>WARNING</strong>: When srun is executed from within salloc or sbatch, there are configurations and options which can result in inconsistent allocations when -c has a value greater than -c on salloc or sbatch.</p>
<p>This option applies to job allocations.</p>
</blockquote>
<h2 id="l-–label-用于打印当前的task-id"><a href="#l-–label-用于打印当前的task-id" class="headerlink" title="-l, –label 用于打印当前的task id"></a><strong>-l</strong>, <strong>–label</strong> 用于打印当前的task id</h2><blockquote>
<p><strong>-l</strong>, <strong>–label</strong></p>
<p><strong>Prepend task number to lines of stdout/err</strong>. The <strong>–label</strong> option will prepend lines of output with the remote task id. This option applies to step allocations.</p>
</blockquote>
<p>用于查看log的时候很有用，不过仅仅当<code>srun</code>时指定的<code>ntasks &gt; 1</code>才会有效，否则都是多个0</p>
<p><img src="/2019/10/09/server-usage/image-20210203164430016.png" alt="image-20210203164430016"></p>
<h1 id="查看任务信息常用命令"><a href="#查看任务信息常用命令" class="headerlink" title="查看任务信息常用命令"></a>查看任务信息常用命令</h1><h2 id="scontrol-show-job-id-查看job详细信息"><a href="#scontrol-show-job-id-查看job详细信息" class="headerlink" title="scontrol show job_id 查看job详细信息"></a><code>scontrol show job_id</code> 查看job详细信息</h2><h2 id="sacct-打印当前用户所有job的信息"><a href="#sacct-打印当前用户所有job的信息" class="headerlink" title="sacct 打印当前用户所有job的信息"></a><code>sacct</code> 打印当前用户所有job的信息</h2><h2 id="sacct-j-job-id-查看指定job的信息"><a href="#sacct-j-job-id-查看指定job的信息" class="headerlink" title="sacct -j job_id 查看指定job的信息"></a><code>sacct -j job_id</code> 查看指定job的信息</h2><h2 id="scontrol-show-node-查看node详细信息"><a href="#scontrol-show-node-查看node详细信息" class="headerlink" title="scontrol show node 查看node详细信息"></a><code>scontrol show node</code> 查看node详细信息</h2><h1 id="sbatch机制的一些理解"><a href="#sbatch机制的一些理解" class="headerlink" title="sbatch机制的一些理解"></a>sbatch机制的一些理解</h1><p>参考</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/54849116">Slurm can’t run more than one sbatch task</a></p>
<p>通过sbatch提交script后，sbatch会首先创建一个名为batch的job step，本质就是 <strong>运行所提交的shell script的操作</strong>。在script中的每一条<code>srun</code>命令，都会创建一个child process（对应一个job step），而这些<code>srun</code>命令 <strong>都只能在sbatch所申请并被salloc的资源中运行（共享（多条srun，可以实现并行？）或者独占（只有一条srun））</strong></p>
<p>如下的命令，则会先申请 <strong>两个进程</strong>，然后分别在<strong>每个进程中</strong>运行一个文件</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token comment">#SBATCH --ntasks=2</span>
srun --ntasks<span class="token operator">=</span><span class="token number">1</span> something.py
srun --ntasks<span class="token operator">=</span><span class="token number">1</span> somethingelse.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>


<h1 id="salloc的用法"><a href="#salloc的用法" class="headerlink" title="salloc的用法"></a>salloc的用法</h1><ul>
<li><p>可以直接<code>salloc --gres=gpu:&lt;number of gpus&gt;  -n&lt;number of processes&gt;</code>来直接创建一个包含&lt;number of gpus&gt; 个gpu和&lt;number of processes&gt; 进程的bash</p>
</li>
<li><p>要在这个bash中访问到GPU资源，<strong>仍然要用 <code>srun</code>命令来获取</strong>，如<code>srun env  | grep CUDA</code></p>
</li>
<li><p>直接输入一条命令，默认则会同时放在&lt;number of processes&gt; 个进程中运行（即运行&lt;number of processes&gt; 次）</p>
</li>
<li><p>shell脚本中，直接用&amp;接起来的算共享一个资源，不用&amp;连接的srun命令则可以实现 <strong>资源的重复使用，不会被占用</strong>。因此若所有srun命令都直接用&amp;接起来，很有可能是 <strong>因为资源不足而报错！</strong></p>
</li>
<li><p>例子：</p>
<p>如下的script，首先<code>salloc --gres=gpu:3 -n2</code> 获得了三个GPU，两个CPU（线程）。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ scontrol show job <span class="token number">9524</span>
<span class="token assign-left variable">JobId</span><span class="token operator">=</span><span class="token number">9524</span> <span class="token assign-left variable">JobName</span><span class="token operator">=</span>bash
   <span class="token assign-left variable">UserId</span><span class="token operator">=</span>weitaotang<span class="token punctuation">(</span><span class="token number">1025</span><span class="token punctuation">)</span> <span class="token assign-left variable">GroupId</span><span class="token operator">=</span>weitaotang<span class="token punctuation">(</span><span class="token number">1025</span><span class="token punctuation">)</span> <span class="token assign-left variable">MCS_label</span><span class="token operator">=</span>N/A
   <span class="token assign-left variable">Priority</span><span class="token operator">=</span><span class="token number">4294899444</span> <span class="token assign-left variable">Nice</span><span class="token operator">=</span><span class="token number">0</span> <span class="token assign-left variable">Account</span><span class="token operator">=</span>users <span class="token assign-left variable">QOS</span><span class="token operator">=</span>normal
   <span class="token assign-left variable">JobState</span><span class="token operator">=</span>RUNNING <span class="token assign-left variable">Reason</span><span class="token operator">=</span>None <span class="token assign-left variable">Dependency</span><span class="token operator">=</span><span class="token punctuation">(</span>null<span class="token punctuation">)</span>
   <span class="token assign-left variable">Requeue</span><span class="token operator">=</span><span class="token number">1</span> <span class="token assign-left variable">Restarts</span><span class="token operator">=</span><span class="token number">0</span> <span class="token assign-left variable">BatchFlag</span><span class="token operator">=</span><span class="token number">0</span> <span class="token assign-left variable">Reboot</span><span class="token operator">=</span><span class="token number">0</span> <span class="token assign-left variable">ExitCode</span><span class="token operator">=</span><span class="token number">0</span>:0
   <span class="token assign-left variable">RunTime</span><span class="token operator">=</span>00:10:51 <span class="token assign-left variable">TimeLimit</span><span class="token operator">=</span>UNLIMITED <span class="token assign-left variable">TimeMin</span><span class="token operator">=</span>N/A
   <span class="token assign-left variable">SubmitTime</span><span class="token operator">=</span><span class="token number">2019</span>-10-10T13:40:14 <span class="token assign-left variable">EligibleTime</span><span class="token operator">=</span><span class="token number">2019</span>-10-10T13:40:14
   <span class="token assign-left variable">AccrueTime</span><span class="token operator">=</span>Unknown
   <span class="token assign-left variable">StartTime</span><span class="token operator">=</span><span class="token number">2019</span>-10-10T13:40:14 <span class="token assign-left variable">EndTime</span><span class="token operator">=</span>Unknown <span class="token assign-left variable">Deadline</span><span class="token operator">=</span>N/A
   <span class="token assign-left variable">PreemptTime</span><span class="token operator">=</span>None <span class="token assign-left variable">SuspendTime</span><span class="token operator">=</span>None <span class="token assign-left variable">SecsPreSuspend</span><span class="token operator">=</span><span class="token number">0</span>
   <span class="token assign-left variable">LastSchedEval</span><span class="token operator">=</span><span class="token number">2019</span>-10-10T13:40:14
   <span class="token assign-left variable">Partition</span><span class="token operator">=</span>defq AllocNode:Sid<span class="token operator">=</span>bcm:29374
   <span class="token assign-left variable">ReqNodeList</span><span class="token operator">=</span><span class="token punctuation">(</span>null<span class="token punctuation">)</span> <span class="token assign-left variable">ExcNodeList</span><span class="token operator">=</span><span class="token punctuation">(</span>null<span class="token punctuation">)</span>
   <span class="token assign-left variable">NodeList</span><span class="token operator">=</span>node02
   <span class="token assign-left variable">BatchHost</span><span class="token operator">=</span>node02
   <span class="token assign-left variable">NumNodes</span><span class="token operator">=</span><span class="token number">1</span> <span class="token assign-left variable">NumCPUs</span><span class="token operator">=</span><span class="token number">2</span> <span class="token assign-left variable">NumTasks</span><span class="token operator">=</span><span class="token number">2</span> CPUs/Task<span class="token operator">=</span><span class="token number">1</span> ReqB:S:C:T<span class="token operator">=</span><span class="token number">0</span>:0:*:*
   <span class="token assign-left variable">TRES</span><span class="token operator">=</span>cpu<span class="token operator">=</span><span class="token number">2</span>,node<span class="token operator">=</span><span class="token number">1</span>,billing<span class="token operator">=</span><span class="token number">2</span>,gres/gpu<span class="token operator">=</span><span class="token number">3</span>
   Socks/Node<span class="token operator">=</span>* NtasksPerN:B:S:C<span class="token operator">=</span><span class="token number">0</span>:0:*:* <span class="token assign-left variable">CoreSpec</span><span class="token operator">=</span>*
   <span class="token assign-left variable">MinCPUsNode</span><span class="token operator">=</span><span class="token number">1</span> <span class="token assign-left variable">MinMemoryNode</span><span class="token operator">=</span><span class="token number">0</span> <span class="token assign-left variable">MinTmpDiskNode</span><span class="token operator">=</span><span class="token number">0</span>
   <span class="token assign-left variable">Features</span><span class="token operator">=</span><span class="token punctuation">(</span>null<span class="token punctuation">)</span> <span class="token assign-left variable">DelayBoot</span><span class="token operator">=</span>00:00:00
   <span class="token assign-left variable">OverSubscribe</span><span class="token operator">=</span>OK <span class="token assign-left variable">Contiguous</span><span class="token operator">=</span><span class="token number">0</span> <span class="token assign-left variable">Licenses</span><span class="token operator">=</span><span class="token punctuation">(</span>null<span class="token punctuation">)</span> <span class="token assign-left variable">Network</span><span class="token operator">=</span><span class="token punctuation">(</span>null<span class="token punctuation">)</span>
   <span class="token assign-left variable">Command</span><span class="token operator">=</span><span class="token punctuation">(</span>null<span class="token punctuation">)</span>
   <span class="token assign-left variable">WorkDir</span><span class="token operator">=</span>/home/weitaotang/noisy_label/GNN/mnist_gnn_pytorch/end_to_end/shell_script
   <span class="token assign-left variable">Power</span><span class="token operator">=</span>
   <span class="token assign-left variable">TresPerNode</span><span class="token operator">=</span>gpu:3
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>直接运行脚本可以看到，第一条<code>srun ./show_device.sh</code>命令因为没有指明进程数，<strong>所以默认用上全部进程（tasks）</strong>，因此<code>STEP_NUM_TASKS</code>为2。运行了两次，两次的输出结果的<code>RANK</code>（<code>SLURM_PROCID</code>）都不一样。而之后的两条srun，则 <strong>共用一个资源空间</strong>，因此可用的GPU都不同，但是RANK都是0（在各自的进程中的相对PID都是0）。</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ <span class="token function">cat</span> test_added_multiprocess.sh
<span class="token comment">#!/bin/sh</span>
<span class="token comment">#srun bash -c 'echo JOB $SLURM_JOB_ID TASK ID $SLURM_TASK_PID CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES'</span>
srun ./show_device.sh
<span class="token comment">#srun --gres=gpu:2 -n1 --exclusive ./show_device.sh &amp;</span>
<span class="token comment">#srun --gres=gpu:1 -n1 --exclusive ./show_device.sh &amp;</span>
srun --gres<span class="token operator">=</span>gpu:2 -n1 ./show_device.sh <span class="token operator">&amp;</span>
srun --gres<span class="token operator">=</span>gpu:1 -n1 ./show_device.sh <span class="token operator">&amp;</span>
<span class="token function">wait</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ ./test_added_multiprocess.sh
<span class="token environment constant">JOB</span> <span class="token number">9524</span> STEP <span class="token number">12</span> Current Rank <span class="token number">0</span> TASK ID <span class="token number">55448</span> STEP_NUM_TASKS <span class="token number">2</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">5,6</span>,7
<span class="token environment constant">JOB</span> <span class="token number">9524</span> STEP <span class="token number">12</span> Current Rank <span class="token number">1</span> TASK ID <span class="token number">55449</span> STEP_NUM_TASKS <span class="token number">2</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">5,6</span>,7
<span class="token environment constant">JOB</span> <span class="token number">9524</span> STEP <span class="token number">13</span> Current Rank <span class="token number">0</span> TASK ID <span class="token number">55478</span> STEP_NUM_TASKS <span class="token number">1</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">5,6</span>
<span class="token environment constant">JOB</span> <span class="token number">9524</span> STEP <span class="token number">14</span> Current Rank <span class="token number">0</span> TASK ID <span class="token number">55485</span> STEP_NUM_TASKS <span class="token number">1</span> <span class="token assign-left variable">CUDA_VISIBLE_DEVICES</span><span class="token operator">=</span><span class="token number">7</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</li>
</ul>
<h1 id="关于每个用户可用GPU数的理解："><a href="#关于每个用户可用GPU数的理解：" class="headerlink" title="关于每个用户可用GPU数的理解："></a>关于每个用户可用GPU数的理解：</h1><ul>
<li>6个可用指的是 <strong>所有job</strong>加起来总共6个可用</li>
<li>即便是在不同terminal中用<code>salloc</code>尝试分配，一旦各个terminal的总数加起来超过6，也不行</li>
<li>因此：<strong>最佳的做法是直接在一个sbatch中求得6个或者在salloc中直接求得6个，然后用job steps的办法实现并行，或者用pytorch等自己的方法实现并行</strong></li>
</ul>
<h1 id="官方的一些例子"><a href="#官方的一些例子" class="headerlink" title="官方的一些例子"></a>官方的一些例子</h1><p><a target="_blank" rel="noopener" href="https://slurm.schedmd.com/cpu_management.html#Section2">CPU Management User and Administrator Guide</a>，主要是CPU资源的分配，不过大差不差，gpu也是也是的思路。可以看看。</p>
<p>比如下面这个，就比较有代表性</p>
<p><img src="/2019/10/09/server-usage/image-20210203162602683.png" alt="image-20210203162602683"></p>
<h1 id="多进程的一些测试："><a href="#多进程的一些测试：" class="headerlink" title="多进程的一些测试："></a>多进程的一些测试：</h1><ul>
<li>如果想不先 <strong>显式地salloc资源，再运行script</strong>，而是直接运行shell script的话：<ol>
<li>使用nccl 后端：<ol>
<li>使用file share：不同node也可</li>
<li>使用tcp：必须得在同一个node上面</li>
</ol>
</li>
<li>使用gloo后端：<ol>
<li>使用file share：不同node都可</li>
<li>使用tcp：必须得在同一个node上面</li>
</ol>
</li>
</ol>
</li>
<li>必须得在每一条<code>srun</code>中 **显式地用&amp;**，保证其在后台运行</li>
<li>使用tcp初始化的话，必须使用 <strong>127.0.0.1（即本地IP）</strong>，因为实验室的计算节点不支持ssh直接IP访问，端口可以任意</li>
</ul>
<h2 id="所有线程都写在一个shell-script里，不加–exclusive"><a href="#所有线程都写在一个shell-script里，不加–exclusive" class="headerlink" title="所有线程都写在一个shell script里，不加–exclusive"></a>所有线程都写在一个shell script里，不加–exclusive</h2><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span>
<span class="token comment">#SBATCH --gres=gpu:3</span>
<span class="token comment">#SBATCH -n 2</span>

<span class="token assign-left variable">work_folder</span><span class="token operator">=</span><span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training"</span>
<span class="token builtin class-name">echo</span> <span class="token string">"<span class="token variable">$work_folder</span>"</span>
srun --gres<span class="token operator">=</span>gpu:2 -n1 python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --init-method tcp://127.0.0.1:23456 --rank <span class="token number">0</span> --world-size <span class="token number">2</span>
srun --gres<span class="token operator">=</span>gpu:2 -n1 python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --init-method tcp://127.0.0.1:23456 --rank <span class="token number">1</span> --world-size <span class="token number">2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>实验表明会stuck在第一个里面（<strong>因为默认的<code>dist.init_process_group</code>的timeout参数，对于gloo backend来说是30分钟，因此会一直等所有进程都初始化完成</strong>）</p>
<p>手动结束第一个进程<code>scancel 9656</code> ，会继续执行第二行srun，但结果一样是stuck在那里。</p>
<p>因此这样是 <strong>无法完成多进程初始化的</strong></p>
<h2 id="所有线程都写在一个shell-script里，加–exclusive"><a href="#所有线程都写在一个shell-script里，加–exclusive" class="headerlink" title="所有线程都写在一个shell script里，加–exclusive"></a>所有线程都写在一个shell script里，加–exclusive</h2><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span>
<span class="token comment">#SBATCH --gres=gpu:3</span>
<span class="token comment">#SBATCH -n 2</span>

<span class="token assign-left variable">work_folder</span><span class="token operator">=</span><span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training"</span>
<span class="token builtin class-name">echo</span> <span class="token string">"<span class="token variable">$work_folder</span>"</span>
srun --gres<span class="token operator">=</span>gpu:2 -n1 --exclusive python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --init-method tcp://127.0.0.1:23456 --rank <span class="token number">0</span> --world-size <span class="token number">2</span>
srun --gres<span class="token operator">=</span>gpu:2 -n1 --exclusive python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --init-method tcp://127.0.0.1:23456 --rank <span class="token number">1</span> --world-size <span class="token number">2</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>超过了系统限制则不行</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ ./run_toy_case.sh
/home/weitaotang/noisy_label/GNN/mnist_gnn_pytorch/end_to_end/distributed_training
srun: job <span class="token number">9658</span> queued and waiting <span class="token keyword">for</span> resources
srun: Job has been cancelled
srun: error: Unable to allocate resources: Job/step already completing or completed
srun: job <span class="token number">9659</span> queued and waiting <span class="token keyword">for</span> resources
srun: Job has been cancelled
srun: Force Terminated job <span class="token number">9659</span>
srun: error: Unable to allocate resources: Job/step already completing or completed
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>对原来的shell script进行修改：调小gpu个数</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span>
<span class="token comment">#SBATCH --gres=gpu:3</span>
<span class="token comment">#SBATCH -n 2</span>

<span class="token assign-left variable">work_folder</span><span class="token operator">=</span><span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training"</span>
<span class="token builtin class-name">echo</span> <span class="token string">"<span class="token variable">$work_folder</span>"</span>
srun --gres<span class="token operator">=</span>gpu:1 -n1 --exclusive python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --init-method tcp://127.0.0.1:23456 --rank <span class="token number">0</span> --world-size <span class="token number">2</span>
srun --gres<span class="token operator">=</span>gpu:1 -n1 --exclusive python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --init-method tcp://127.0.0.1:23456 --rank <span class="token number">1</span> --world-size <span class="token number">2</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>依旧不行</p>
<p>结果：无法实现多进程</p>
<h2 id="正确做法1：分terminal进行，用命令行参数对shell文件输入进行更改"><a href="#正确做法1：分terminal进行，用命令行参数对shell文件输入进行更改" class="headerlink" title="正确做法1：分terminal进行，用命令行参数对shell文件输入进行更改"></a>正确做法1：分terminal进行，用命令行参数对shell文件输入进行更改</h2><ul>
<li><p>需要注意：若使用<code>tcp</code>的初始化方式，则 <strong>一定要在127.0.0.1上进行</strong>，端口可以任意，协议可以用<code>nccl</code></p>
</li>
<li><p><code>srun</code>中的进程数<code>-n --ntasks</code> <strong>必须为1</strong>，否则子进程必报错（即 <strong>最后还只是留下一个进程</strong>），因为除了一开始的job step那个进程外，其余进程初始化时都会发现端口被占用</p>
</li>
<li><p><code>srun</code> 中当GPU数多于1时，也很容易不行（<strong>因为此时很容易把两个任务分到不同的node上，则对应的localhost都不同了</strong>），<strong>但若在<code>srun</code>中强制指定在同一台机子上，则可以，因为都是在同一个node上运行的</strong></p>
</li>
<li><p>参考的shell script 与 toy_case.py中的端口设定如下(强制都在node3上，端口随便定)</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span>
<span class="token comment">#SBATCH --gres=gpu:3</span>
<span class="token comment">#SBATCH -n 2</span>

<span class="token assign-left variable">work_folder</span><span class="token operator">=</span><span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training"</span>
<span class="token builtin class-name">echo</span> <span class="token string">"<span class="token variable">$work_folder</span>"</span>
srun --gres<span class="token operator">=</span>gpu:2  --pty -w node03 python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --backend nccl --init_port <span class="token number">37082</span>  --rank <span class="token variable">$1</span> --world-size <span class="token number">2</span>
<span class="token function">wait</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">args<span class="token punctuation">.</span>init_method <span class="token operator">=</span> <span class="token string">'tcp://'</span> <span class="token operator">+</span> <span class="token string">'127.0.0.1'</span> <span class="token operator">+</span> <span class="token string">':'</span> <span class="token operator">+</span> args<span class="token punctuation">.</span>init_port<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


</li>
</ul>
<h2 id="正确做法2：-用salloc分配资源，然后shell-script中实现并行"><a href="#正确做法2：-用salloc分配资源，然后shell-script中实现并行" class="headerlink" title="正确做法2： 用salloc分配资源，然后shell script中实现并行"></a>正确做法2： 用salloc分配资源，然后shell script中实现并行</h2><ul>
<li>先通过<code>salloc</code>分配资源，然后可以通过设定<code>--exclusive</code>来保证各个线程之间互不冲突</li>
<li>此时若直接用<code>import ipdb \ ipdb.set_trace()</code>来debug是肯定不行的，必须得用<code>python -m ipdb</code>来debug。<strong>但是需要注意的是，虽然两个srun指令都有ipdb，但是在terminal中显示得就只有一个的感觉，因此若要debug，还是通过在不同terminal开不同的process来实现比较好</strong></li>
</ul>
<p>​        否则容易报错：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">(</span>tfpt_py3<span class="token punctuation">)</span> <span class="token punctuation">[</span>weitaotang@bcm shell_script<span class="token punctuation">]</span>$ ./run_toy_case.sh
/home/weitaotang/noisy_label/GNN/mnist_gnn_pytorch/end_to_end/distributed_training
<span class="token operator">></span> /home/weitaotang/noisy_label/GNN/mnist_gnn_pytorch/end_to_end/distributed_training/toy_case.py<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>main<span class="token punctuation">(</span><span class="token punctuation">)</span>
     <span class="token number">31</span>     ipdb.set_trace<span class="token punctuation">(</span><span class="token punctuation">)</span>
---<span class="token operator">></span> <span class="token number">32</span>     parser <span class="token operator">=</span> argparse.ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
     <span class="token number">33</span>     parser.add_argument<span class="token punctuation">(</span><span class="token string">'--backend'</span>, <span class="token assign-left variable">type</span><span class="token operator">=</span>str, <span class="token assign-left variable">default</span><span class="token operator">=</span><span class="token string">'gloo'</span>, <span class="token assign-left variable">help</span><span class="token operator">=</span><span class="token string">'Name of the backend to use.'</span><span class="token punctuation">)</span>

<span class="token operator">></span> /home/weitaotang/noisy_label/GNN/mnist_gnn_pytorch/end_to_end/distributed_training/toy_case.py<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>main<span class="token punctuation">(</span><span class="token punctuation">)</span>
     <span class="token number">31</span>     ipdb.set_trace<span class="token punctuation">(</span><span class="token punctuation">)</span>
---<span class="token operator">></span> <span class="token number">32</span>     parser <span class="token operator">=</span> argparse.ArgumentParser<span class="token punctuation">(</span><span class="token punctuation">)</span>
     <span class="token number">33</span>     parser.add_argument<span class="token punctuation">(</span><span class="token string">'--backend'</span>, <span class="token assign-left variable">type</span><span class="token operator">=</span>str, <span class="token assign-left variable">default</span><span class="token operator">=</span><span class="token string">'gloo'</span>, <span class="token assign-left variable">help</span><span class="token operator">=</span><span class="token string">'Name of the backend to use.'</span><span class="token punctuation">)</span>

ipdb<span class="token operator">></span>
Exiting Debugger.
srun: error: node01: task <span class="token number">0</span>: Exited with <span class="token builtin class-name">exit</span> code <span class="token number">1</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<ul>
<li><p>&amp; 的使用：能够保证那一条command在 <strong>后台中</strong> 运行，从而能够实现 <strong>多线程</strong>。如果在shell脚本中不加 &amp; ，则容易导致 <strong>一直stuck在上一条命令中</strong>，没办法开启多进程。<strong>因此切记一定要加&amp;！！！</strong></p>
</li>
<li><p>不建议加<code>--pty</code>容易导致后面出问题</p>
</li>
<li><p>各条srun之间的<code>CUDA_VISIBLE_DEVICES</code>都是exclusive的（即一定 <strong>各不相同</strong>），但是每一条srun中的<code>torch.cuda.current_device()</code>默认还是 <strong>从0开始</strong></p>
<h4 id="更新："><a href="#更新：" class="headerlink" title="更新："></a>更新：</h4><ul>
<li><p>根据<a target="_blank" rel="noopener" href="https://github.com/mknoxnv/ubuntu-slurm/issues/7">Unable to share GPU</a>，SLURM会给每个job steps都分配 <strong>独立的GPU</strong>，因此不同job steps之间的<code>CUDA_VISIBLE_DEVICES</code>都参数都是不同的（哪怕 <strong>不加exclusive</strong>），所以这么做的话，会导致各个GPU之间无法通讯。**(已经不对了！！)**</p>
<blockquote>
<p>Slurm can assign jobs to particular GPUs on a node. Multiple jobs per node. Slurm will not assign multiple jobs to the same GPU hardware. This is documented under generic resource scheduling (GRES). <a target="_blank" rel="noopener" href="https://slurm.schedmd.com/gres.html">https://slurm.schedmd.com/gres.html</a></p>
</blockquote>
</li>
<li><p>因此，如果要实现并行计算，<strong>只能srun一条命令，然后在这条命令中生成多个进程来实现（通过spawn或者launch）(已经不对了！！)</strong></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>是可以通过这个这个方法实现，但是要注意以下几个点：</p>
<ol>
<li><p>申请资源的时候<code>--gres=gpu:</code> <code>-n</code> 一定要相等，<strong>且恰好等于希望并行的数目（即一个GPU对应一个进程</strong></p>
</li>
<li><p>各条子srun命令运行的时候，**一定要显式地加入<code>-n1</code>**参数，来保证他们不会互不共享进程（否则容易出现Address Used）的error</p>
</li>
<li><p>参考例子如下(文件开头显式地确定了所需的gpu数）：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token comment">#SBATCH --gres=gpu:3</span>
<span class="token comment">#SBATCH -n 3</span>
<span class="token builtin class-name">echo</span> <span class="token string">"Number of gpus is <span class="token variable">$1</span>"</span>

<span class="token keyword">for</span> <span class="token variable"><span class="token punctuation">((</span> r <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> r <span class="token operator">&lt;=</span> "$<span class="token number">1</span>"<span class="token punctuation">;</span> r<span class="token operator">++</span> <span class="token punctuation">))</span></span><span class="token punctuation">;</span> <span class="token keyword">do</span>
  srun --gres<span class="token operator">=</span>gpu:1 -n1 --pty python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/cifar10_mixmatch_only_dist.py"</span> <span class="token punctuation">\</span>
    --rank <span class="token string">"<span class="token variable">$r</span>"</span> --world-size <span class="token string">"<span class="token variable">$1</span>"</span> --init-method tcp://127.0.0.1:29874 <span class="token operator">&amp;</span>
<span class="token keyword">done</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>对应的salloc命令如下：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">salloc --gres<span class="token operator">=</span>gpu:3 -n3<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>




</li>
</ol>
</li>
</ul>
<h2 id="正确做法3：分terminal执行，init-method-用file"><a href="#正确做法3：分terminal执行，init-method-用file" class="headerlink" title="正确做法3：分terminal执行，init_method 用file://"></a>正确做法3：分terminal执行，init_method 用<code>file://</code></h2><ul>
<li>注意<code>file://</code>后面还要接多一个<code>\</code>，如<code>file:///home/weitaotang/sharefiles</code></li>
<li>sharefiles必须不存在！！（最好手动删除，虽然pytorch也能自动删），但是对应的directory必须存在！！（即pytorch不会帮你创建directory）</li>
<li>建议不加<code>--pty</code>，否则容易报错</li>
<li>不建议加<code>--exclusive</code> ，很容易被queued</li>
</ul>
<h2 id="正确做法4：-使用sbatch，提交shell-script"><a href="#正确做法4：-使用sbatch，提交shell-script" class="headerlink" title="正确做法4： 使用sbatch，提交shell script"></a>正确做法4： 使用sbatch，提交shell script</h2><ul>
<li><p>初始化参数可以在shell script开头通过添加<code>#SBATCH --options</code>来实现。<strong>但是像普通shell文件那样没办法直接通过命令参数输入</strong></p>
</li>
<li><p>init_method为tcp或者file share都行</p>
</li>
<li><p><code>--exclusive</code>可加可不加</p>
</li>
<li><p>例子：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/sh</span>
<span class="token comment">#SBATCH --gres=gpu:4</span>
<span class="token comment">#SBATCH -n 2</span>
<span class="token comment">#SBATCH -o toy_case_sbatch.txt</span>

<span class="token assign-left variable">work_folder</span><span class="token operator">=</span><span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training"</span>
<span class="token builtin class-name">echo</span> <span class="token string">"<span class="token variable">$work_folder</span>"</span>
<span class="token comment">#srun --gres=gpu:2  -n1 -v --exclusive python "$(dirname "$PWD")/distributed_training/toy_case.py" --init-method file:///home/weitaotang/sharefiles --backend gloo --init_port 37082  --rank 0 --world-size 2 &amp;</span>
<span class="token comment">#srun --gres=gpu:2  -n1 -v --exclusive python "$(dirname "$PWD")/distributed_training/toy_case.py" --init-method file:///home/weitaotang/sharefiles --backend gloo --init_port 37082  --rank 1 --world-size 2 &amp;</span>
srun --gres<span class="token operator">=</span>gpu:2  -n1  python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --init-method tcp://127.0.0.1:37082 --backend nccl --rank <span class="token number">0</span> --world-size <span class="token number">2</span> <span class="token operator">&amp;</span>
srun --gres<span class="token operator">=</span>gpu:2  -n1  python <span class="token string">"<span class="token variable"><span class="token variable">$(</span><span class="token function">dirname</span> <span class="token string">"<span class="token environment constant">$PWD</span>"</span><span class="token variable">)</span></span>/distributed_training/toy_case.py"</span> --init-method tcp://127.0.0.1:37082 --backend nccl --rank <span class="token number">1</span> --world-size <span class="token number">2</span> <span class="token operator">&amp;</span>
<span class="token function">wait</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>





</li>
</ul>
<h1 id="srun-debug"><a href="#srun-debug" class="headerlink" title="srun debug"></a>srun debug</h1><h2 id="法1：通过ipdb"><a href="#法1：通过ipdb" class="headerlink" title="法1：通过ipdb"></a>法1：通过ipdb</h2><ul>
<li><p>可以通过<code>-m ipdb</code>的方式进行<strong>（首推这个）</strong>，也可以通过在需要停下来的地方加<code>ipdb.set_trace()</code></p>
</li>
<li><p>加上<code>-c continue</code>之后，则能够保证其 一直运行到有错的地方位置，<strong>然后在errors的位置停下来</strong></p>
</li>
<li><p>例子：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">srun --gres<span class="token operator">=</span>gpu:1 python -m ipdb xxx.py args<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


</li>
</ul>
<h2 id="法2：通过Pycharm的Remote-Dedugging-强推！！"><a href="#法2：通过Pycharm的Remote-Dedugging-强推！！" class="headerlink" title="法2：通过Pycharm的Remote Dedugging(强推！！)"></a>法2：通过Pycharm的Remote Dedugging(强推！！)</h2><ul>
<li>参考：</li>
</ul>
<ol>
<li>官方教程：<a target="_blank" rel="noopener" href="https://www.jetbrains.com/help/pycharm/remote-debugging-with-product.html#remote-debug-config">Remote debugging with the Python remote debug server configuration</a></li>
<li>这个帖子说的非常清楚，详参这个：<a target="_blank" rel="noopener" href="https://stackoverflow.com/a/7061956">How do I start up remote debugging with PyCharm?</a></li>
<li>还可以参考这个关于docker的：<a target="_blank" rel="noopener" href="https://stackoverflow.com/a/24326540">From inside of a Docker container, how do I connect to the localhost of the machine?</a></li>
<li>详细中文解释：<a target="_blank" rel="noopener" href="https://debugtalk.com/post/remote-debugging-with-pycharm/">利用 PyCharm 进行 Python 远程调试</a></li>
</ol>
<ul>
<li><p>server和client的定义：这里实际上是通过 <strong>本地机扮演server，remote的server扮演client的角度</strong>，来实现remote debugging的。</p>
<blockquote>
<p>PyCharm (or your ide of choice) acts as the “server” and your application is the “client”; so you start the server first - tell the IDE to ‘debug’ - then run the client - which is some code with the <code>settrace</code> statement in it. When your python code hits the <code>settrace</code> it connects to the server - pycharm - and starts feeding it the debug data.</p>
</blockquote>
</li>
<li><p>具体的原理就是通过ssh，当在client（服务器上）运行的code到达了<code>settrace</code>的地方时，则会通过制定好的地址和端口进行通讯，把服务器的数据回传到本地的IDE中（即server端）</p>
</li>
<li><p>set_trace的具体参数详解，<code>pydevd_pycharm.settrace(&lt;host name&gt;, port=&lt;port number&gt;)</code>：</p>
<ol>
<li><code>host name</code>指的是运行在本地的host，<strong>具体而言指的是本机IDE在局域网中的IP（可通过ipconfig）查看</strong></li>
</ol>
</li>
</ul>
<ol start="2">
<li><code>port name</code>默认是0，表示随机生成一个端口，也可以指定为IDE本机上空闲的端口</li>
<li><code>suspend</code> 默认为True，即会在<code>settrace</code>的地方停下，如果设为<code>False</code>，<strong>则会在断点或者exception处停下（如果在View Breakpoint中设定了any exception）</strong></li>
</ol>
<ul>
<li><p>具体使用方法：</p>
<ol>
<li><p>到Configuration—-Edit Configuration—-Python Remote Debug中新增一个Configuration</p>
</li>
<li><p>设定好本地ide在局域网中的IP和port之后，根据图中的提示在remote server上安装对应的版本的pydev-pycharm（注意！<strong>一定要安装对应版本的pydev-pycharm</strong>，命令中<code>~=</code>部分后的数字即版本号）</p>
</li>
<li><p>设定好映射的路径，注意只用<code>base_name</code>（即待运行文件所在folder的映射）即可。</p>
</li>
<li><p>在待运行的文件开头插入对应的代码片段</p>
</li>
<li><p>本地IDE开启调试模式，remote server上用<code>srun</code>等工具运行命令，则可进入调试模式。</p>
</li>
</ol>
</li>
</ul>
<h2 id="开启GPU调试"><a href="#开启GPU调试" class="headerlink" title="开启GPU调试"></a>开启GPU调试</h2><ul>
<li><p>需要使用内网穿透（推荐Frp）</p>
</li>
<li><p>最直接的，通过natfrp来穿透，转发流量。需要注意，<strong>实验室的主机必须开启UDP转发，才可以实现服务器直接连接到natfrp的服务</strong></p>
</li>
<li><p>实验室的主机通过socat同时开启tcp和UDP转发</p>
</li>
<li><p>实验室的主机无需开启frp client，直接在需要调试的机子开启frp client，然后pycharm的debug监那个端口就可以了。参考配置如下：</p>
<p>   1， natfrp上开启tcp转发，本地地址为127.0.0.1，本地端口为13531，远程端口（natfrp服务器的）端口为12382</p>
<ol start="2">
<li><p>实验室的主机socat转发，监听39581，然后转发到natfrp的服务器（如现在经常用宿迁服务器 <code>s25.natfrp.org</code> ），端口为12382</p>
</li>
<li><p>本地的debug的电脑上开启frp client即可，pycharm debugger的address用127.0.0.1，port用13531</p>
</li>
<li><p>python文件中设置连接到实验室的主机(10.8.15.170, port=39581)</p>
</li>
<li><p>先开启本地的server服务监听端口，然后通过Mobaxterm把脚本跑起来就可以了</p>
</li>
</ol>
</li>
</ul>
<h2 id="20210129-update-最新的简单方法！！无需在第三台主机上开启frp"><a href="#20210129-update-最新的简单方法！！无需在第三台主机上开启frp" class="headerlink" title="20210129 update(最新的简单方法！！无需在第三台主机上开启frp)"></a>20210129 update(最新的简单方法！！无需在第三台主机上开启frp)</h2><p>参考：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://dinghuang.github.io/2019/01/07/%E5%9F%BA%E4%BA%8EFrp%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%E5%AE%9E%E7%8E%B0%E6%9C%AC%E5%9C%B0%E6%9C%8D%E5%8A%A1%E5%99%A8/">基于Frp内网穿透反向代理的端口转发实现本地服务器</a></li>
<li><a target="_blank" rel="noopener" href="https://gofrp.org/docs/examples/ssh/">通过 SSH 访问内网机器</a></li>
</ol>
<p>最简单，通过一个<strong>有公网IP的VPS转发流量</strong>即可。即：</p>
<ol>
<li>直接在一个公网的VPS上开启frp，本地主机开启frpc连接公网的VPS，本地主机的<code>frpc</code>中设置为<code>ssh</code></li>
<li>然后在代码中的host设置为公网VPS的IP</li>
</ol>
<p>参考设置：</p>
<ul>
<li>本地主机所用的<code>frpc.ini</code>：</li>
</ul>
<pre class="line-numbers language-ini" data-language="ini"><code class="language-ini"><span class="token selector">[common]</span>
<span class="token constant">server_addr</span> <span class="token attr-value"><span class="token punctuation">=</span> xx.xx.xx.xx</span>
<span class="token constant">server_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 7000</span>

<span class="token selector">[ssh]</span>
<span class="token constant">type</span> <span class="token attr-value"><span class="token punctuation">=</span> tcp</span>
<span class="token constant">local_ip</span> <span class="token attr-value"><span class="token punctuation">=</span> 127.0.0.1</span>
<span class="token constant">local_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 21342</span>
<span class="token constant">remote_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 21342</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<ul>
<li>公网vps所用的<code>frps.ini</code></li>
</ul>
<pre class="line-numbers language-ini" data-language="ini"><code class="language-ini"><span class="token selector">[common]</span>
<span class="token constant">bind_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 7000</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>


<ul>
<li>代码中的debug设置：</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pydevd_pycharm<span class="token punctuation">.</span>settrace<span class="token punctuation">(</span><span class="token string">'194.87.232.23'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">21342</span><span class="token punctuation">,</span> stdoutToServer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> stderrToServer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> suspend<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<p>后期还可以考虑使用stcp，参考 <a target="_blank" rel="noopener" href="https://gofrp.org/docs/examples/stcp/">安全地暴露内网服务</a></p>
<h2 id="20210213-frp使用stcp"><a href="#20210213-frp使用stcp" class="headerlink" title="20210213 frp使用stcp"></a>20210213 frp使用stcp</h2><p>服务端（即公网的VPS）的设置和上面的直接ssh的类似：</p>
<pre class="line-numbers language-ini" data-language="ini"><code class="language-ini"><span class="token selector">[common]</span>
<span class="token constant">bind_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 36130</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>


<p>此时本地机和服务器上都是作为client，其中本地机可以认为是<strong>被暴露内网</strong>的机子，而服务器则作为<strong>访问本地机服务</strong>的外部机器（即visitor）。因此两个客户端的设置分别如下：</p>
<p>服务器上：</p>
<pre class="line-numbers language-ini" data-language="ini"><code class="language-ini"><span class="token selector">[common]</span>
<span class="token constant">server_addr</span> <span class="token attr-value"><span class="token punctuation">=</span> xxx.xx.xx.xx</span>
<span class="token constant">server_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 36130</span>

<span class="token selector">[secret_ssh_visitor]</span>
<span class="token comment"># frpc role visitor -> frps -> frpc role server</span>
<span class="token constant">role</span> <span class="token attr-value"><span class="token punctuation">=</span> visitor</span>
<span class="token constant">type</span> <span class="token attr-value"><span class="token punctuation">=</span> stcp</span>
<span class="token comment"># the server name you want to visitor</span>
<span class="token constant">server_name</span> <span class="token attr-value"><span class="token punctuation">=</span> secret_ssh</span>
<span class="token comment"># 即key，可以随机生成</span>
<span class="token constant">sk</span> <span class="token attr-value"><span class="token punctuation">=</span> yX7dp@ApWU8fA4iR</span>
<span class="token comment"># connect this address to visitor stcp server</span>
<span class="token comment"># 即在服务器上，在代码中访问对应的端口即可创建到本地机的连接</span>
<span class="token constant">bind_addr</span> <span class="token attr-value"><span class="token punctuation">=</span> 127.0.0.1</span>
<span class="token constant">bind_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 21342</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>本地机：</p>
<pre class="line-numbers language-ini" data-language="ini"><code class="language-ini"><span class="token selector">[common]</span>
<span class="token constant">server_addr</span> <span class="token attr-value"><span class="token punctuation">=</span> 194.87.232.23</span>
<span class="token constant">server_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 36130</span>

<span class="token selector">[secret_ssh]</span>
<span class="token comment"># If the type is secret tcp, remote_port is useless</span>
<span class="token comment"># Who want to connect local port should deploy another frpc with stcp proxy and role is visitor</span>
<span class="token constant">type</span> <span class="token attr-value"><span class="token punctuation">=</span> stcp</span>
<span class="token comment"># sk used for authentication for visitors</span>
<span class="token constant">sk</span> <span class="token attr-value"><span class="token punctuation">=</span> yX7dp@ApWU8fA4iR</span>
<span class="token constant">local_ip</span> <span class="token attr-value"><span class="token punctuation">=</span> 127.0.0.1</span>
<span class="token constant">local_port</span> <span class="token attr-value"><span class="token punctuation">=</span> 21342</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>在代码中添加如下的即可：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">pydevd_pycharm<span class="token punctuation">.</span>settrace<span class="token punctuation">(</span><span class="token string">'127.0.0.1'</span><span class="token punctuation">,</span> port<span class="token operator">=</span><span class="token number">21342</span><span class="token punctuation">,</span> stdoutToServer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> stderrToServer<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> suspend<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<p>但是由于本质上被暴露的机子是本地机，服务器在直接的ssh连接中直接访问公网的vps，不会有任何信息的泄露，因此推荐直接用 [20210129 update(最新的简单方法！！无需在第三台主机上开启frp)](#20210129 update(最新的简单方法！！无需在第三台主机上开启frp))中的方法</p>
<h1 id="和tmux或者screen的结合"><a href="#和tmux或者screen的结合" class="headerlink" title="和tmux或者screen的结合"></a>和tmux或者screen的结合</h1><p>对于tmux来说，是没办法直接获取到source的allocation的：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 开启tmux</span>
❯ salloc -w node05 -n <span class="token number">2</span> tmux

<span class="token comment"># 进入tmux之后，可以看到ntask没有受限</span>
❯ srun -l --ntasks <span class="token number">8</span> -c <span class="token number">1</span> <span class="token function">hostname</span>
srun: job <span class="token number">99824</span> queued and waiting <span class="token keyword">for</span> resources
srun: job <span class="token number">99824</span> has been allocated resources
<span class="token number">3</span>: node05
<span class="token number">1</span>: node05
<span class="token number">4</span>: node05
<span class="token number">6</span>: node05
<span class="token number">2</span>: node05
<span class="token number">7</span>: node05
<span class="token number">0</span>: node05
<span class="token number">5</span>: node05<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>但是如果直接用zsh就可以，没有问题，被限制了</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">❯ salloc -w node05 -n <span class="token number">2</span> <span class="token function">zsh</span>
salloc: Pending job allocation <span class="token number">99825</span>
salloc: job <span class="token number">99825</span> queued and waiting <span class="token keyword">for</span> resources
salloc: job <span class="token number">99825</span> has been allocated resources
salloc: Granted job allocation <span class="token number">99825</span>
<span class="token comment"># 上述的`ntasks`限制了</span>
❯ srun -l --ntasks <span class="token number">8</span> -c <span class="token number">1</span> <span class="token function">hostname</span>
srun: error: Unable to create step <span class="token keyword">for</span> job <span class="token number">99825</span>: More processors requested than permitted
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h2 id="折中办法：salloc后，先srun一个shell到计算节点上，再在上面开启tmux"><a href="#折中办法：salloc后，先srun一个shell到计算节点上，再在上面开启tmux" class="headerlink" title="折中办法：salloc后，先srun一个shell到计算节点上，再在上面开启tmux"></a>折中办法：<code>salloc</code>后，先<code>srun</code>一个shell到计算节点上，再在上面开启tmux</h2><p>这样之后就能够顺利在<code>node05</code>上用上tmux了，而且也可以通过<code>srun</code>来控制node或者cpu的数量</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">❯ srun -l --ntasks <span class="token number">8</span> -c <span class="token number">1</span> <span class="token function">hostname</span>
❯ srun -n1 --pty <span class="token function">zsh</span>
❯ <span class="token builtin class-name">echo</span> <span class="token variable">$SLURM_NTASKS</span>
<span class="token number">1</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2019/10/09/server-usage/image-20210203154216825.png" alt="image-20210203154216825"></p>
<p>可以看到此时这个zsh只有一个tasks quota了</p>
<p>直接输入tmux，能够在<code>node05</code>上成功开启，并且<code>ntasks</code>数量受上一层的<code>srun</code>控制</p>
<p><img src="/2019/10/09/server-usage/image-20210203154354328.png" alt="image-20210203154354328"></p>
<p>整理下来即：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 指定好节点，以及对应所需的tasks数</span>
salloc --gres<span class="token operator">=</span>gpu:1 -w nodename -n <span class="token number">6</span> <span class="token function">zsh</span>
<span class="token comment"># 通过srun获取gpu资源，运行对应的程序</span>
srun --gres<span class="token operator">=</span>gpu:1 -t <span class="token number">4320</span> --pty <span class="token function">zsh</span>

<span class="token comment">## Optional 开启tmux，不过不推荐直接在zsh中开启tmux</span>
tmux<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>推荐可以用screen，因为能够共享<code>salloc</code>的信息</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token comment"># 指定好节点，以及对应所需的tasks数</span>
salloc --gres<span class="token operator">=</span>gpu:1 -w nodename -n <span class="token number">6</span> <span class="token function">zsh</span>
<span class="token function">screen</span>
<span class="token comment"># 通过srun获取gpu资源，运行对应的程序</span>
srun --gres<span class="token operator">=</span>gpu:1 -t <span class="token number">4320</span> --pty <span class="token function">zsh</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2019/10/09/server-usage/image-20210204122127015.png" alt="image-20210204122127015"></p>
<h1 id="sbatch-job-array的一些尝试"><a href="#sbatch-job-array的一些尝试" class="headerlink" title="sbatch job array的一些尝试"></a>sbatch job array的一些尝试</h1><p>参考 <a target="_blank" rel="noopener" href="https://slurm.schedmd.com/job_array.html">Job Array Support</a></p>
<p>其实用法很简单，就是在提交<code>sbatch</code>时候，加多一个<code>--array</code>参数即可。最后对应的<code>job_id</code>就会有一个后缀，比如下面的就是<code>--array=1-4</code>的结果</p>
<p><img src="/2019/10/09/server-usage/image-20210203164218802.png" alt="image-20210203164218802"></p>
<p>下面的实验都是基本基于下面这个脚本</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token comment">#SBATCH --ntasks=4</span>
<span class="token comment">## more options</span>

srun -l --ntasks <span class="token number">1</span> -c <span class="token number">1</span> <span class="token function">bash</span> -c <span class="token string">"echo running <span class="token variable">$SLURM_ARRAY_JOB_ID</span>\/<span class="token variable">$SLURM_ARRAY_TASK_ID</span> sub tasks;sleep 10"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>先说总结：</p>
<ul>
<li>本质上job array等价于<strong>自动提交多个<code>sbatch</code>，并添加了一些环境变量</strong>，所以<code>sbatch</code>脚本中的shebang的限制只限制array中的每个sub task</li>
</ul>
<h2 id="shebang只限制每个sub-task，而salloc分配的资源限制总的task数"><a href="#shebang只限制每个sub-task，而salloc分配的资源限制总的task数" class="headerlink" title="shebang只限制每个sub task，而salloc分配的资源限制总的task数"></a><code>shebang</code>只限制每个<code>sub task</code>，而<code>salloc</code>分配的资源限制总的task数</h2><p>一开始<code>salloc -n 8 --pty zsh</code>开启一个新的shell，可以看到总的<code>tasks</code>数即3</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">❯ <span class="token builtin class-name">echo</span> <span class="token variable">$SLURM_NTASKS</span>
<span class="token number">8</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>默认用上面<code>#SBATCH --ntasks=4</code>，然后使用<code>sbatch --array=1-12 interpret_task.sh</code>提交任务，可以看到<code>#SBATCH --ntasks=4</code>的限制对整个job array是无效的，但是总的<code>ntasks=8</code>还是起效的，<code>99875_[8-12]</code>作为一个job step占用一个task</p>
<p><img src="/2019/10/09/server-usage/image-20210203165250224.png" alt="image-20210203165250224"></p>
<p>修改之后的脚本，此时<code>shebang</code>就起作用了，因此此时等价于每条<code>srun</code>（每个所提交的脚本文件）的tasks数都被限制为3</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token comment">#SBATCH --ntasks=3</span>
<span class="token comment">## more options</span>
<span class="token comment">#srun --ntasks 8 -c 2 bash -c "echo hello; sleep 10"</span>
<span class="token comment">#srun -l --ntasks 4 -c 2 hostname</span>

srun -l --ntasks <span class="token number">2</span> -c <span class="token number">2</span> <span class="token function">bash</span> -c <span class="token string">"echo running <span class="token variable">$SLURM_ARRAY_JOB_ID</span>\/<span class="token variable">$SLURM_ARRAY_TASK_ID</span> sub tasks;sleep 10"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>运行提示资源不足，直接被杀</p>
<p><img src="/2019/10/09/server-usage/image-20210204132040677.png" alt="image-20210204132040677"></p>
<h2 id="srun提交的ntasks-2-cpu-per-task-2"><a href="#srun提交的ntasks-2-cpu-per-task-2" class="headerlink" title="srun提交的ntasks=2 cpu-per-task=2"></a><code>srun</code>提交的<code>ntasks=2 cpu-per-task=2</code></h2><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sbatch --array<span class="token operator">=</span><span class="token number">1</span>-12 interpret_task.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><img src="/2019/10/09/server-usage/image-20210204132708964.png" alt="image-20210204132708964"></p>
<p>由于一开始<code>salloc</code>获得的<code>ntasks=8</code>，而默认情况下<code>cpu-per-task=1</code>，因此等价于<strong>总共获取了8个cpu</strong>。</p>
<p>而每一条<code>srun</code>都需要4个cpu，因此执行的时候等价于<strong>每次只能执行两条<code>srun</code>指令</strong></p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token comment">#SBATCH --ntasks=4</span>
<span class="token comment">## more options</span>

srun -l --ntasks <span class="token number">2</span> -c <span class="token number">2</span> <span class="token function">bash</span> -c <span class="token string">"echo running <span class="token variable">$SLURM_ARRAY_JOB_ID</span>\/<span class="token variable">$SLURM_ARRAY_TASK_ID</span> sub tasks;sleep 10"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h2 id="提交gpu任务"><a href="#提交gpu任务" class="headerlink" title="提交gpu任务"></a>提交gpu任务</h2><p>注意由于job array是基于<code>sbatch</code>的，因此需要gpu任务的需要在<code>shebang</code>中指定，而不应该在<code>srun</code>中指定</p>
<p>即下面的才是正确的</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>
<span class="token comment">#SBATCH --ntasks=3</span>
<span class="token comment">#SBATCH --gres=gpu:1 # 正确做法</span>
<span class="token comment">## more options</span>
<span class="token comment">#srun --ntasks 8 -c 2 bash -c "echo hello; sleep 10"</span>
<span class="token comment">#srun -l --ntasks 4 -c 2 hostname</span>

<span class="token comment">#错误写法</span>
<span class="token comment"># srun -l --gres=gpu:1 --ntasks 1 -c 1 bash -c "echo running $SLURM_ARRAY_JOB_ID\/$SLURM_ARRAY_TASK_ID sub tasks;sleep 10"</span>
srun -l --ntasks <span class="token number">1</span> -c <span class="token number">1</span> <span class="token function">bash</span> -c <span class="token string">"echo running <span class="token variable">$SLURM_ARRAY_JOB_ID</span>\/<span class="token variable">$SLURM_ARRAY_TASK_ID</span> sub tasks;sleep 10"</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>但是由于job array基于<code>sbatch</code>，所以执行下面的两条指令不能成功：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">sbatch --array<span class="token operator">=</span><span class="token number">1</span>-12 interpret_task_gpu.sh
<span class="token comment"># or 手动限制每次执行的任务数为1</span>
sbatch --array<span class="token operator">=</span><span class="token number">1</span>-12%1 interpret_task_gpu.sh<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p><img src="/2019/10/09/server-usage/image-20210204135353813.png" alt="image-20210204135353813"></p>
<p>必须得关了一开始<code>salloc</code>，退出这个job，直接在管理节点上提交job array才行</p>
<p><img src="/2019/10/09/server-usage/image-20210204135145683.png" alt="image-20210204135145683"></p>
<h2 id="针对submitit的一些理解"><a href="#针对submitit的一些理解" class="headerlink" title="针对submitit的一些理解"></a>针对submitit的一些理解</h2><p>这里主要指的是hydra的。hydra的submitit插件在multirun的情况下，默认就是基于sbatch的job array，所以同样的如果在<code>salloc</code>之后，然后在<code>config</code>中指定了<code>gres=gpu:1</code>，一样会因为资源被限制而无法执行</p>
<p><img src="/2019/10/09/server-usage/image-20210204140928955.png" alt="image-20210204140928955"></p>
<p>对应的<code>config.yaml</code>和自动生成的<code>submission.sh</code>：</p>
<pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">defaults</span><span class="token punctuation">:</span>
  <span class="token punctuation">-</span> <span class="token key atrule">hydra/launcher</span><span class="token punctuation">:</span> submitit_slurm

<span class="token key atrule">task</span><span class="token punctuation">:</span> <span class="token number">1</span>
<span class="token key atrule">hydra</span><span class="token punctuation">:</span>
  <span class="token key atrule">launcher</span><span class="token punctuation">:</span>
    <span class="token key atrule">cpus_per_task</span><span class="token punctuation">:</span> <span class="token number">1</span>
    <span class="token key atrule">tasks_per_node</span><span class="token punctuation">:</span> <span class="token number">1</span>
    <span class="token key atrule">partition</span><span class="token punctuation">:</span> defq
    <span class="token key atrule">array_parallelism</span><span class="token punctuation">:</span> <span class="token number">1</span>
    <span class="token key atrule">additional_parameters</span><span class="token punctuation">:</span>
      <span class="token key atrule">nodelist</span><span class="token punctuation">:</span> <span class="token string">"node04"</span>
      <span class="token key atrule">time</span><span class="token punctuation">:</span> <span class="token number">1440</span>
      <span class="token key atrule">gres</span><span class="token punctuation">:</span> gpu<span class="token punctuation">:</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token comment"># Parameters</span>
<span class="token comment">#SBATCH --array=0-2%1</span>
<span class="token comment">#SBATCH --cpus-per-task=1</span>
<span class="token comment">#SBATCH --error=/home/weitaotang/package/hydra/plugins/hydra_submitit_launcher/example/multirun/2021-02-04/14-09-11/.submitit/%A_%a/%A_%a_0_log.err</span>
<span class="token comment">#SBATCH --gres=gpu:1</span>
<span class="token comment">#SBATCH --job-name=my_app</span>
<span class="token comment">#SBATCH --mem=4GB</span>
<span class="token comment">#SBATCH --nodelist=node04</span>
<span class="token comment">#SBATCH --nodes=1</span>
<span class="token comment">#SBATCH --ntasks-per-node=1</span>
<span class="token comment">#SBATCH --open-mode=append</span>
<span class="token comment">#SBATCH --output=/home/weitaotang/package/hydra/plugins/hydra_submitit_launcher/example/multirun/2021-02-04/14-09-11/.submitit/%A_%a/%A_%a_0_log.out</span>
<span class="token comment">#SBATCH --partition=defq</span>
<span class="token comment">#SBATCH --signal=USR1@120</span>
<span class="token comment">#SBATCH --time=1440</span>
<span class="token comment">#SBATCH --wckey=submitit</span>

<span class="token comment"># command</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">SUBMITIT_EXECUTOR</span><span class="token operator">=</span>slurm
srun --output /home/weitaotang/package/hydra/plugins/hydra_submitit_launcher/example/multirun/2021-02-04/14-09-11/.submitit/%A_%a/%A_%a_%t_log.out --error /home/weitaotang/package/hydra/plugins/hydra_submitit_launcher/example/multirun/2021-02-04/14-09-11/.submitit/%A_%a/%A_%a_%t_log.err --unbuffered /home/weitaotang/.conda/envs/multimodal/bin/python -u -m submitit.core._submit /home/weitaotang/package/hydra/plugins/hydra_submitit_launcher/example/multirun/2021-02-04/14-09-11/.submitit/%j
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>同样是需要推出<code>salloc</code>重新提交。可以看到在设置了<code>array_parallelism: 1</code>后，这时候就能够成功执行了</p>
<p><img src="/2019/10/09/server-usage/image-20210204141251563.png" alt="image-20210204141251563"></p>
<p><img src="/2019/10/09/server-usage/image-20210204141357108.png" alt="image-20210204141357108"></p>
<h1 id="submitit重跑job-array中失败的任务，并保存到原目录"><a href="#submitit重跑job-array中失败的任务，并保存到原目录" class="headerlink" title="submitit重跑job array中失败的任务，并保存到原目录"></a>submitit重跑job array中失败的任务，并保存到原目录</h1><p>关键点：</p>
<ul>
<li>删除上一次失败的任务，或者把他们改名（比如在开头添加<code>failed</code>）</li>
<li>修改一开始的参数</li>
</ul>
<p>具体步骤如下：</p>
<p>首先找到submiti自动生成的脚本（通常在对应<code>sweep</code> folder的<code>.submitit</code>下），比如下面这个</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token comment"># Parameters</span>
<span class="token comment">#SBATCH --array=0-7%8</span>
<span class="token comment">#SBATCH --cpus-per-task=4</span>
<span class="token comment">#SBATCH --error=/home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%A_%a/%A_%a_0_log.err</span>
<span class="token comment">#SBATCH --exclude=node01,node02,node03</span>
<span class="token comment">#SBATCH --gres=gpu:1</span>
<span class="token comment">#SBATCH --job-name=enterface_ieg_train</span>
<span class="token comment">#SBATCH --mem=12GB</span>
<span class="token comment">#SBATCH --nodes=1</span>
<span class="token comment">#SBATCH --ntasks-per-node=1</span>
<span class="token comment">#SBATCH --open-mode=append</span>
<span class="token comment">#SBATCH --output=/home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%A_%a/%A_%a_0_log.out</span>
<span class="token comment">#SBATCH --partition=defq</span>
<span class="token comment">#SBATCH --qos=high</span>
<span class="token comment">#SBATCH --signal=USR1@120</span>
<span class="token comment">#SBATCH --time=4320</span>
<span class="token comment">#SBATCH --wckey=submitit</span>

<span class="token comment"># command</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">SUBMITIT_EXECUTOR</span><span class="token operator">=</span>slurm
srun --output /home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%A_%a/%A_%a_%t_log.out --error /home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%A_%a/%A_%a_%t_log.err --unbuffered /home/weitaotang/.conda/envs/multimodal_new/bin/python -u -m submitit.core._submit /home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%j<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p>然后修改开头的job array，比如失败的任务是0,5，则可以直接修改第一行为<code>#SBATCH --array=0,5</code></p>
<p>同时还可以加入<code>requeue</code>让其自动失败重跑。最后的脚本如下</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token shebang important">#!/bin/bash</span>

<span class="token comment"># Parameters</span>
<span class="token comment">#SBATCH --array=0,5</span>
<span class="token comment">#SBATCH --requeue</span>
<span class="token comment">#SBATCH --cpus-per-task=4</span>
<span class="token comment">#SBATCH --error=/home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%A_%a/%A_%a_0_log.err</span>
<span class="token comment">#SBATCH --exclude=node01,node02,node03</span>
<span class="token comment">#SBATCH --gres=gpu:1</span>
<span class="token comment">#SBATCH --job-name=enterface_ieg_train</span>
<span class="token comment">#SBATCH --mem=12GB</span>
<span class="token comment">#SBATCH --nodes=1</span>
<span class="token comment">#SBATCH --ntasks-per-node=1</span>
<span class="token comment">#SBATCH --open-mode=append</span>
<span class="token comment">#SBATCH --output=/home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%A_%a/%A_%a_0_log.out</span>
<span class="token comment">#SBATCH --partition=defq</span>
<span class="token comment">#SBATCH --qos=high</span>
<span class="token comment">#SBATCH --signal=USR1@120</span>
<span class="token comment">#SBATCH --time=4320</span>
<span class="token comment">#SBATCH --wckey=submitit</span>

<span class="token comment"># command</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">SUBMITIT_EXECUTOR</span><span class="token operator">=</span>slurm
srun --output /home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%A_%a/%A_%a_%t_log.out --error /home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%A_%a/%A_%a_%t_log.err --unbuffered /home/weitaotang/.conda/envs/multimodal_new/bin/python -u -m submitit.core._submit /home/weitaotang/multimodal/pytorch_hydra_results_temp/multirun/2021-03-10/21-21-13-enterface_reuters_ieg_best_original_all_rerun_lr0.001/.submitit/%j<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://khalitt.github.io">Walter</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://khalitt.github.io/2019/10/09/server-usage/">https://khalitt.github.io/2019/10/09/server-usage/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/server/">server</a><a class="post-meta__tags" href="/tags/submitit/">submitit</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/gh/overtrue/share.js@master/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/10/11/linux-usage/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">linux使用基础</div></div></a></div><div class="next-post pull-right"><a href="/2019/10/07/pycharm-usage/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">pycharm</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Slurm%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%B8%B8%E7%94%A8%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">Slurm服务器的常用概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#20200203-update"><span class="toc-number">1.1.</span> <span class="toc-text">20200203 update</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Slurm-%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="toc-number">2.</span> <span class="toc-text">Slurm 服务器使用常用命令</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81interactive-mode%E7%9A%84bash%EF%BC%9A%E2%80%93pty"><span class="toc-number">2.1.</span> <span class="toc-text">支持interactive mode的bash：–pty</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#salloc-srun-sbatch%E5%8C%BA%E5%88%AB"><span class="toc-number">2.2.</span> <span class="toc-text">salloc srun sbatch区别</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B5%8C%E5%A5%97srun%E8%BF%90%E8%A1%8C%E5%B9%B6%E4%B8%8D%E5%8F%AF%E8%A1%8C"><span class="toc-number">3.</span> <span class="toc-text">嵌套srun运行并不可行</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#srun%EF%BC%88salloc%EF%BC%89%E7%9A%84%E5%B8%B8%E7%94%A8arguments"><span class="toc-number">4.</span> <span class="toc-text">srun（salloc）的常用arguments</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#n-%E2%80%93ntasks-%EF%BC%9A%E8%BF%90%E8%A1%8C-lt-number-gt-%E4%B8%AA%E4%BB%BB%E5%8A%A1-%E8%BF%90%E8%A1%8C-lt-number-gt-%E4%B8%AA%E4%BB%BB%E5%8A%A1"><span class="toc-number">4.1.</span> <span class="toc-text">-n, –ntasks&#x3D;：运行&lt;number&gt;个任务(运行&lt;number&gt;个任务)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SLURM-PROCID-%EF%BC%9A%E5%8F%AF%E7%90%86%E8%A7%A3%E4%B8%BA%E5%AD%90%E8%BF%9B%E7%A8%8B%E7%9A%84%E7%9B%B8%E5%AF%B9ID-SLURM-TASK-PID-%E5%88%99%E6%98%AF%E5%9C%A8%E7%B3%BB%E7%BB%9F%E4%B8%AD%E5%AE%9E%E9%99%85%E7%9A%84PID"><span class="toc-number">4.2.</span> <span class="toc-text">SLURM_PROCID ：可理解为子进程的相对ID SLURM_TASK_PID 则是在系统中实际的PID</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gres-lt-list-gt-%E6%98%AF%E9%92%88%E5%AF%B9%E6%AF%8F%E4%B8%AAnode%E6%9D%A5%E8%AE%B2%E7%9A%84"><span class="toc-number">4.3.</span> <span class="toc-text">gres&#x3D;&lt;list&gt; 是针对每个node来讲的</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%93exclusive-%E5%B8%B8%E7%94%A8%E4%BA%8E%E4%BF%9D%E8%AF%81%E6%AF%8F%E4%B8%AAjob-step%E9%83%BD%E8%83%BD%E5%A4%9F%E7%8B%AC%E5%8D%A0%E8%B5%84%E6%BA%90%EF%BC%8C%E4%B8%8D%E4%BC%9A%E4%BA%92%E7%9B%B8%E5%BD%B1%E5%93%8D"><span class="toc-number">4.4.</span> <span class="toc-text">–exclusive 常用于保证每个job step都能够独占资源，不会互相影响</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#c-%E2%80%93cpus-per-task-%E6%AF%8F%E4%B8%AAtask%E8%83%BD%E5%A4%9F%E8%8E%B7%E5%8F%96%E5%88%B0%E7%9A%84cpu%E6%95%B0%E3%80%82%E5%AF%B9%E5%BA%94%E4%BA%8Enum-workers"><span class="toc-number">4.5.</span> <span class="toc-text">-c, –cpus-per-task 每个task能够获取到的cpu数。对应于num_workers</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#l-%E2%80%93label-%E7%94%A8%E4%BA%8E%E6%89%93%E5%8D%B0%E5%BD%93%E5%89%8D%E7%9A%84task-id"><span class="toc-number">4.6.</span> <span class="toc-text">-l, –label 用于打印当前的task id</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E4%BB%BB%E5%8A%A1%E4%BF%A1%E6%81%AF%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4"><span class="toc-number">5.</span> <span class="toc-text">查看任务信息常用命令</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#scontrol-show-job-id-%E6%9F%A5%E7%9C%8Bjob%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF"><span class="toc-number">5.1.</span> <span class="toc-text">scontrol show job_id 查看job详细信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sacct-%E6%89%93%E5%8D%B0%E5%BD%93%E5%89%8D%E7%94%A8%E6%88%B7%E6%89%80%E6%9C%89job%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="toc-number">5.2.</span> <span class="toc-text">sacct 打印当前用户所有job的信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sacct-j-job-id-%E6%9F%A5%E7%9C%8B%E6%8C%87%E5%AE%9Ajob%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="toc-number">5.3.</span> <span class="toc-text">sacct -j job_id 查看指定job的信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scontrol-show-node-%E6%9F%A5%E7%9C%8Bnode%E8%AF%A6%E7%BB%86%E4%BF%A1%E6%81%AF"><span class="toc-number">5.4.</span> <span class="toc-text">scontrol show node 查看node详细信息</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sbatch%E6%9C%BA%E5%88%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3"><span class="toc-number">6.</span> <span class="toc-text">sbatch机制的一些理解</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#salloc%E7%9A%84%E7%94%A8%E6%B3%95"><span class="toc-number">7.</span> <span class="toc-text">salloc的用法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E6%AF%8F%E4%B8%AA%E7%94%A8%E6%88%B7%E5%8F%AF%E7%94%A8GPU%E6%95%B0%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%9A"><span class="toc-number">8.</span> <span class="toc-text">关于每个用户可用GPU数的理解：</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%98%E6%96%B9%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BE%8B%E5%AD%90"><span class="toc-number">9.</span> <span class="toc-text">官方的一些例子</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E8%BF%9B%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E6%B5%8B%E8%AF%95%EF%BC%9A"><span class="toc-number">10.</span> <span class="toc-text">多进程的一些测试：</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%80%E6%9C%89%E7%BA%BF%E7%A8%8B%E9%83%BD%E5%86%99%E5%9C%A8%E4%B8%80%E4%B8%AAshell-script%E9%87%8C%EF%BC%8C%E4%B8%8D%E5%8A%A0%E2%80%93exclusive"><span class="toc-number">10.1.</span> <span class="toc-text">所有线程都写在一个shell script里，不加–exclusive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%80%E6%9C%89%E7%BA%BF%E7%A8%8B%E9%83%BD%E5%86%99%E5%9C%A8%E4%B8%80%E4%B8%AAshell-script%E9%87%8C%EF%BC%8C%E5%8A%A0%E2%80%93exclusive"><span class="toc-number">10.2.</span> <span class="toc-text">所有线程都写在一个shell script里，加–exclusive</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E7%A1%AE%E5%81%9A%E6%B3%951%EF%BC%9A%E5%88%86terminal%E8%BF%9B%E8%A1%8C%EF%BC%8C%E7%94%A8%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0%E5%AF%B9shell%E6%96%87%E4%BB%B6%E8%BE%93%E5%85%A5%E8%BF%9B%E8%A1%8C%E6%9B%B4%E6%94%B9"><span class="toc-number">10.3.</span> <span class="toc-text">正确做法1：分terminal进行，用命令行参数对shell文件输入进行更改</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E7%A1%AE%E5%81%9A%E6%B3%952%EF%BC%9A-%E7%94%A8salloc%E5%88%86%E9%85%8D%E8%B5%84%E6%BA%90%EF%BC%8C%E7%84%B6%E5%90%8Eshell-script%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%B9%B6%E8%A1%8C"><span class="toc-number">10.4.</span> <span class="toc-text">正确做法2： 用salloc分配资源，然后shell script中实现并行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%EF%BC%9A"><span class="toc-number">10.4.0.1.</span> <span class="toc-text">更新：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E7%A1%AE%E5%81%9A%E6%B3%953%EF%BC%9A%E5%88%86terminal%E6%89%A7%E8%A1%8C%EF%BC%8Cinit-method-%E7%94%A8file"><span class="toc-number">10.5.</span> <span class="toc-text">正确做法3：分terminal执行，init_method 用file:&#x2F;&#x2F;</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E7%A1%AE%E5%81%9A%E6%B3%954%EF%BC%9A-%E4%BD%BF%E7%94%A8sbatch%EF%BC%8C%E6%8F%90%E4%BA%A4shell-script"><span class="toc-number">10.6.</span> <span class="toc-text">正确做法4： 使用sbatch，提交shell script</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#srun-debug"><span class="toc-number">11.</span> <span class="toc-text">srun debug</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%951%EF%BC%9A%E9%80%9A%E8%BF%87ipdb"><span class="toc-number">11.1.</span> <span class="toc-text">法1：通过ipdb</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%952%EF%BC%9A%E9%80%9A%E8%BF%87Pycharm%E7%9A%84Remote-Dedugging-%E5%BC%BA%E6%8E%A8%EF%BC%81%EF%BC%81"><span class="toc-number">11.2.</span> <span class="toc-text">法2：通过Pycharm的Remote Dedugging(强推！！)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%80%E5%90%AFGPU%E8%B0%83%E8%AF%95"><span class="toc-number">11.3.</span> <span class="toc-text">开启GPU调试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20210129-update-%E6%9C%80%E6%96%B0%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95%EF%BC%81%EF%BC%81%E6%97%A0%E9%9C%80%E5%9C%A8%E7%AC%AC%E4%B8%89%E5%8F%B0%E4%B8%BB%E6%9C%BA%E4%B8%8A%E5%BC%80%E5%90%AFfrp"><span class="toc-number">11.4.</span> <span class="toc-text">20210129 update(最新的简单方法！！无需在第三台主机上开启frp)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#20210213-frp%E4%BD%BF%E7%94%A8stcp"><span class="toc-number">11.5.</span> <span class="toc-text">20210213 frp使用stcp</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%92%8Ctmux%E6%88%96%E8%80%85screen%E7%9A%84%E7%BB%93%E5%90%88"><span class="toc-number">12.</span> <span class="toc-text">和tmux或者screen的结合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%98%E4%B8%AD%E5%8A%9E%E6%B3%95%EF%BC%9Asalloc%E5%90%8E%EF%BC%8C%E5%85%88srun%E4%B8%80%E4%B8%AAshell%E5%88%B0%E8%AE%A1%E7%AE%97%E8%8A%82%E7%82%B9%E4%B8%8A%EF%BC%8C%E5%86%8D%E5%9C%A8%E4%B8%8A%E9%9D%A2%E5%BC%80%E5%90%AFtmux"><span class="toc-number">12.1.</span> <span class="toc-text">折中办法：salloc后，先srun一个shell到计算节点上，再在上面开启tmux</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sbatch-job-array%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%9D%E8%AF%95"><span class="toc-number">13.</span> <span class="toc-text">sbatch job array的一些尝试</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#shebang%E5%8F%AA%E9%99%90%E5%88%B6%E6%AF%8F%E4%B8%AAsub-task%EF%BC%8C%E8%80%8Csalloc%E5%88%86%E9%85%8D%E7%9A%84%E8%B5%84%E6%BA%90%E9%99%90%E5%88%B6%E6%80%BB%E7%9A%84task%E6%95%B0"><span class="toc-number">13.1.</span> <span class="toc-text">shebang只限制每个sub task，而salloc分配的资源限制总的task数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#srun%E6%8F%90%E4%BA%A4%E7%9A%84ntasks-2-cpu-per-task-2"><span class="toc-number">13.2.</span> <span class="toc-text">srun提交的ntasks&#x3D;2 cpu-per-task&#x3D;2</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E4%BA%A4gpu%E4%BB%BB%E5%8A%A1"><span class="toc-number">13.3.</span> <span class="toc-text">提交gpu任务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%92%88%E5%AF%B9submitit%E7%9A%84%E4%B8%80%E4%BA%9B%E7%90%86%E8%A7%A3"><span class="toc-number">13.4.</span> <span class="toc-text">针对submitit的一些理解</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#submitit%E9%87%8D%E8%B7%91job-array%E4%B8%AD%E5%A4%B1%E8%B4%A5%E7%9A%84%E4%BB%BB%E5%8A%A1%EF%BC%8C%E5%B9%B6%E4%BF%9D%E5%AD%98%E5%88%B0%E5%8E%9F%E7%9B%AE%E5%BD%95"><span class="toc-number">14.</span> <span class="toc-text">submitit重跑job array中失败的任务，并保存到原目录</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Walter</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>