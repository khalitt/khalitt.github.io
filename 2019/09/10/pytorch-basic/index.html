<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>pytorch的基本操作 | null</title><meta name="keywords" content="pytorch,basic"><meta name="author" content="Walter"><meta name="copyright" content="Walter"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="tensor转化为listtensor.tolist即可。如果是一个scalar，则用tensor.item(). dilation的含义参考： dilated convolution 查看梯度 用hook参考： how-to-print-models-parameters-with-its-name-and-requires-grad-value 先获取各层的parameter，然后逐层regi">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch的基本操作">
<meta property="og:url" content="https://khalitt.github.io/2019/09/10/pytorch-basic/index.html">
<meta property="og:site_name">
<meta property="og:description" content="tensor转化为listtensor.tolist即可。如果是一个scalar，则用tensor.item(). dilation的含义参考： dilated convolution 查看梯度 用hook参考： how-to-print-models-parameters-with-its-name-and-requires-grad-value 先获取各层的parameter，然后逐层regi">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2019-09-10T02:43:27.000Z">
<meta property="article:modified_time" content="2021-02-07T07:13:02.407Z">
<meta property="article:author" content="Walter">
<meta property="article:tag" content="pytorch">
<meta property="article:tag" content="basic">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://khalitt.github.io/2019/09/10/pytorch-basic/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"5100G1WE6M","apiKey":"c2a0f74ed768aa33e0c961937aa244bc","indexName":"prob_khalitt_github_io2","hits":{"per_page":6},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-02-07 15:13:02'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/null" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">50</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">44</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">20</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-musicW"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/"></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-musicW"></i><span> Music</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch的基本操作</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-09-10T02:43:27.000Z" title="Created 2019-09-10 10:43:27">2019-09-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-02-07T07:13:02.407Z" title="Updated 2021-02-07 15:13:02">2021-02-07</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">5.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>27min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="tensor转化为list"><a href="#tensor转化为list" class="headerlink" title="tensor转化为list"></a>tensor转化为list</h2><p><code>tensor.tolist</code>即可。如果是一个scalar，则用<code>tensor.item()</code>.</p>
<h2 id="dilation的含义"><a href="#dilation的含义" class="headerlink" title="dilation的含义"></a>dilation的含义</h2><p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/43637179">dilated convolution</a></p>
<h2 id="查看梯度"><a href="#查看梯度" class="headerlink" title="查看梯度"></a>查看梯度</h2><ol>
<li><h3 id="用hook"><a href="#用hook" class="headerlink" title="用hook"></a>用hook</h3><p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/how-to-print-models-parameters-with-its-name-and-requires-grad-value/10778">how-to-print-models-parameters-with-its-name-and-requires-grad-value</a></p>
<p>先获取各层的parameter，然后逐层register_hook。</p>
<p>关键在于named_paramters()，这个能够返回各层参数的name。.paramters只能返回一个不带对应name的parameter generator。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">hook</span><span class="token punctuation">(</span>grad<span class="token punctuation">,</span> param_name<span class="token punctuation">)</span><span class="token punctuation">:</span>
    logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'Paramter:&#123;&#125; | \n grad:&#123;&#125; '</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>param_name<span class="token punctuation">,</span> grad<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

handle <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>name<span class="token punctuation">,</span> param<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">:</span>
        <span class="token keyword">def</span> <span class="token function">b_h</span><span class="token punctuation">(</span>grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> hook<span class="token punctuation">(</span>grad<span class="token punctuation">,</span> name<span class="token punctuation">)</span>
        h <span class="token operator">=</span> param<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>b_h<span class="token punctuation">)</span>
        handle<span class="token punctuation">.</span>append<span class="token punctuation">(</span>h<span class="token punctuation">)</span>
        logger<span class="token punctuation">.</span>debug<span class="token punctuation">(</span><span class="token string">'&#123;&#125; param needs grad'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>




</li>
</ol>
<h2 id="NLLLoss与CrossEntropyLoss"><a href="#NLLLoss与CrossEntropyLoss" class="headerlink" title="NLLLoss与CrossEntropyLoss"></a>NLLLoss与CrossEntropyLoss</h2><h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h3><ul>
<li><p>输入应该是log softmax 的 $N \times C$ 的矩阵（或者更多），target应该是<strong>non</strong> one hot（categorical）的$N$维向量。</p>
</li>
<li><p>weight默认为none，<strong>即默认所有类的weight都是1，注意这里所指的weight不需要sum为1，因为最后求mean的时候回把weight.sum()作为分母</strong>。但若自己设定，则必须为$C$ 维的向量，每一个element对应那个类的weight，然后对应的weight会直接作用到对应位置的log prob上（直接相乘）。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
target <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
log_prob <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>weight<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token punctuation">,</span>reduction <span class="token operator">=</span> <span class="token string">'none'</span><span class="token punctuation">)</span>
log_prob
<span class="token operator">>></span><span class="token operator">></span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4076</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4076</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4076</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>log_prob<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
loss
<span class="token operator">>></span><span class="token operator">></span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.4815</span><span class="token punctuation">,</span> <span class="token number">0.5630</span><span class="token punctuation">,</span> <span class="token number">0.4076</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

</li>
<li><p>如果reduction为none，则输出应该是对应位置的-log prob（<strong>即变回正数</strong>），最小化这个正数，让其趋近于0，<strong>等价于让原来对应位置的prob趋近于1</strong></p>
</li>
<li><p>如果reduction为mean，weight为none，则直接求mean。否则还要除于weight的和，然后在把weighted negative log loss加起来。</p>
</li>
<li><p>直接sum的话就是直接加起来</p>
</li>
</ul>
<h3 id="巧用"><a href="#巧用" class="headerlink" title="巧用"></a>巧用</h3><p>虽然要求的输入是log softmax的结果，但是 <strong>本质上就只是把input上target对应位置的量抽出来，然后添加负号，最后根据reduction得到最后的loss</strong>，因此下面的两段本质是一样的：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'sum'</span><span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>logits<span class="token punctuation">[</span>mask_selected_true<span class="token punctuation">]</span><span class="token punctuation">,</span> eval_label_rm<span class="token punctuation">[</span>mask_selected_true<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">one_hot_eval_label <span class="token operator">=</span> make_one_hot<span class="token punctuation">(</span>eval_label_rm<span class="token punctuation">,</span> num_class<span class="token punctuation">)</span>
loss <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>logits<span class="token punctuation">,</span> one_hot_eval_label<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span>mask_selected_true<span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>测试默认的reduction选项：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">(</span>loss <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">0.2</span><span class="token operator">+</span><span class="token number">.4</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span>tensor<span class="token punctuation">(</span><span class="token number">0.9076</span><span class="token punctuation">)</span>
c2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>weight<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>
l2 <span class="token operator">=</span> c2<span class="token punctuation">(</span>log_prob<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
l2
<span class="token operator">>></span><span class="token operator">></span>tensor<span class="token punctuation">(</span><span class="token number">0.9076</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><ul>
<li><p>本质只是把log_softmax 和 NLLoss结合，参数的设置包括性质都和和NLLoss是一样的。只是输入可以直接输入logits</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
target <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
log_prob <span class="token operator">=</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>weight<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> reduction <span class="token operator">=</span> <span class="token string">'none'</span><span class="token punctuation">)</span>
log_prob
<span class="token operator">>></span><span class="token operator">></span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4076</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4076</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4076</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4076</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
loss<span class="token operator">=</span>criterion<span class="token punctuation">(</span>a<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
loss
<span class="token operator">>></span><span class="token operator">></span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.4815</span><span class="token punctuation">,</span> <span class="token number">0.5630</span><span class="token punctuation">,</span> <span class="token number">0.4076</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>默认的reduction选项和NLLoss的一样：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">c2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>weight<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.4</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>
l2 <span class="token operator">=</span> c2<span class="token punctuation">(</span>a<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
l2
<span class="token operator">>></span><span class="token operator">></span>tensor<span class="token punctuation">(</span><span class="token number">0.9076</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>


</li>
</ul>
<h2 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a>乘法</h2><ul>
<li><p>element-wise相乘：<code>torch.mul()</code> or <code>torch.Tensor.mul()</code></p>
</li>
<li><p>vector: <code>torch.matmul()</code> or <code>torch.mul()</code></p>
</li>
<li><p>matrix: </p>
<ol>
<li><code>torch.matmul() </code>带broadcast</li>
<li><code>torch.mm()</code>  or <code>torch.Tensor.mm()</code>不带broadcast</li>
<li><code>torch.bmm()</code> or <code>torch.Tensor.bmm()</code> batch 的不带broadcast</li>
</ol>
</li>
</ul>
<h3 id="torch-where-torch-Tensor-where"><a href="#torch-where-torch-Tensor-where" class="headerlink" title="torch.where() / torch.Tensor.where()"></a>torch.where() / torch.Tensor.where()</h3><p>torch.where(condition, input, other)</p>
<ul>
<li>condition, input, other 可以不同size，只要可以broadcast就行了</li>
<li>input if condition else other</li>
</ul>
<h2 id="获取idx-torch-nonzero"><a href="#获取idx-torch-nonzero" class="headerlink" title="获取idx: torch.nonzero"></a>获取idx: <code>torch.nonzero</code></h2><p>但是要记得squeeze()，因为最后获得的是和原来一样size的tensor</p>
<h2 id="获得validation（切分dataset"><a href="#获得validation（切分dataset" class="headerlink" title="获得validation（切分dataset)"></a>获得validation（切分dataset)</h2><h3 id="random-split"><a href="#random-split" class="headerlink" title="random_split"></a>random_split</h3><ul>
<li>但是可能会导致数据不平衡</li>
</ul>
<h3 id="SubsetRandomSampler"><a href="#SubsetRandomSampler" class="headerlink" title="SubsetRandomSampler"></a>SubsetRandomSampler</h3><ul>
<li>需要自己重构</li>
<li>配合sklearn的StratifiedShuffleSplit</li>
</ul>
<h3 id="自己实现"><a href="#自己实现" class="headerlink" title="自己实现"></a>自己实现</h3><ul>
<li>配合sklearn的StratifiedShuffleSplit和StratifiedKFold参考 <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets">how-do-i-split-a-custom-dataset-into-training-and-test-datasets</a>思路，因为StratifiedShuffleSplit只能得到两个set(train 和 test)，<strong>因此可以通过split两次来得到三个balanced的set（即对train set再split一次）</strong></li>
</ul>
<h2 id="torchvision中的transform"><a href="#torchvision中的transform" class="headerlink" title="torchvision中的transform"></a>torchvision中的transform</h2><ul>
<li>如果不用<code>transforms.Compose</code>来把好几个transform拼接起来，则需要按照由class得到instance的方法先得到对应transform的instance，然后再使用。</li>
<li>除了<code>transforms.ToTensor()</code>可以直接作用再ndarray上，其余很对均要求输入的<strong>PIL格式的image</strong>，如<code>RandomCrop</code>, <code>RandomHorizontalFilp</code>, <code>Normalize</code>等。</li>
</ul>
<h2 id="Dataloader同时输出batch-对应的index"><a href="#Dataloader同时输出batch-对应的index" class="headerlink" title="Dataloader同时输出batch 对应的index"></a>Dataloader同时输出batch 对应的index</h2><h3 id="通过更改Dataset的-getitem"><a href="#通过更改Dataset的-getitem" class="headerlink" title="通过更改Dataset的__getitem__"></a>通过更改Dataset的<code>__getitem__</code></h3><p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/how-to-retrieve-the-sample-indices-of-a-mini-batch/7948/12"><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/how-to-retrieve-the-sample-indices-of-a-mini-batch/7948">How to retrieve the sample indices of a mini-batch</a></a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> DataLoader

<span class="token keyword">class</span> <span class="token class-name">MyDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>cifar10 <span class="token operator">=</span> datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">'YOUR_PATH'</span><span class="token punctuation">,</span>
                                        download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                                        train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                        transform<span class="token operator">=</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        data<span class="token punctuation">,</span> target <span class="token operator">=</span> self<span class="token punctuation">.</span>cifar10<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
        
        <span class="token comment"># Your transformations here (or set it in CIFAR10)</span>
        
        <span class="token keyword">return</span> data<span class="token punctuation">,</span> target<span class="token punctuation">,</span> index

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>cifar10<span class="token punctuation">)</span>

dataset <span class="token operator">=</span> MyDataset<span class="token punctuation">(</span><span class="token punctuation">)</span>
loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span>
                    batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                    shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                    num_workers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> batch_idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>data<span class="token punctuation">,</span> target<span class="token punctuation">,</span> idx<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Batch idx &#123;&#125;, dataset index &#123;&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
        batch_idx<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<h2 id="Infinite-Dataloader"><a href="#Infinite-Dataloader" class="headerlink" title="Infinite Dataloader"></a>Infinite Dataloader</h2><p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/infinite-dataloader/17903/7">infinite-dataloader</a></p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/54359243/implementing-an-infinite-loop-dataset-dataloader-in-pytorch">implementing-an-infinite-loop-dataset-dataloader-in-pytorch</a></p>
<ul>
<li><p>本质上Dataloader输出的就是一个iterable的object，因此可以通过iter() + next() 来实现无限循环</p>
</li>
<li><p><strong>但是更佳的方式是IterableDataset</strong>，但是这个Dataset就没有shuffle等内容，需要自己控制。**注意一定要重载<code>__iter__</code>**，为实现infinite也是通过重载这个实现的，每一个epoch只会得到一个iterator。</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FakeIterableDataset</span><span class="token punctuation">(</span>IterableDataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> data
        self<span class="token punctuation">.</span>count <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Count:&#123;&#125;'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>count<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>count<span class="token operator">+=</span><span class="token number">1</span>
        <span class="token keyword">return</span> <span class="token builtin">iter</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>data<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

fake_dataset <span class="token operator">=</span> FakeIterableDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ld <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>fake_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> item <span class="token keyword">in</span> ld<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">Count:0
tensor([0, 1, 2])
tensor([3, 4, 5])
tensor([6, 7, 8])
tensor([9, 0, 1])
tensor([2, 3, 4])
tensor([5, 6, 7])
tensor([8, 9])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</li>
</ul>
<h2 id="当Dataset-getitem-返回一个dict时"><a href="#当Dataset-getitem-返回一个dict时" class="headerlink" title="当Dataset.__getitem__返回一个dict时"></a>当<code>Dataset.__getitem__</code>返回一个dict时</h2><ul>
<li><p>返回tuple的时候很好理解， batch就是等于zip起来</p>
</li>
<li><p>而当Dataset的<code>__getitem__</code>返回dict的时候，<strong>则是返回一个dict，dict的key一样，只是value变成batch</strong>，而不是<strong>多个dict</strong></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FakeDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>data <span class="token operator">=</span> data
    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">&#123;</span><span class="token string">'idx'</span><span class="token punctuation">:</span>index<span class="token punctuation">,</span> <span class="token string">'data'</span><span class="token punctuation">:</span>self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">&#125;</span>
    
fake_dataset <span class="token operator">=</span> FakeDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
ld <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>fake_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> item <span class="token keyword">in</span> ld<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">&#123;&#39;idx&#39;: tensor([0, 1, 2]), &#39;data&#39;: tensor([ 0, 10, 20])&#125;
&#123;&#39;idx&#39;: tensor([3, 4, 5]), &#39;data&#39;: tensor([30, 40, 50])&#125;
&#123;&#39;idx&#39;: tensor([6, 7, 8]), &#39;data&#39;: tensor([60, 70, 80])&#125;
&#123;&#39;idx&#39;: tensor([9]), &#39;data&#39;: tensor([90])&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>


</li>
</ul>
<h2 id="Softmax的dim"><a href="#Softmax的dim" class="headerlink" title="Softmax的dim"></a>Softmax的dim</h2><ul>
<li><p>dim参数的作用，就是让 <strong>除了该dim以外的dim求和都为1</strong>。对于多维的数据$(S_1, S_2, S_3, \cdots, S_n)$，<code>softmax(dim=i)</code>则等价于$(S_1 \times S_2 \times S_3, \cdots, S_i, \cdots,S_{i+1} \times \cdots \S_n)$ 除了第<code>i</code>维外其余两个维度的任意一个idx，求sum都等于1</p>
</li>
<li><p>如一个 (J, K, L)维度的数据，若<code>softmax(dim=0)</code>则(:, k, l) = 1, k, l为各自维度的index，<strong>可以理解为：三位的数据，使得对z轴求和各个数据，结果为1</strong></p>
</li>
<li><p>以下为`softmax(dim=)</p>
</li>
<li><pre><code class="python">alpha.shape
Out[27]: torch.Size([10, 10, 2])
alpha[:,1,1]
Out[23]: 
tensor([0.0875, 0.0865, 0.0935, 0.1091, 0.1013, 0.1169, 0.1084, 0.0963, 0.0961,
        0.1045], grad_fn=&lt;SelectBackward&gt;)
alpha.view(10, -1).sum(0)
Out[26]: 
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000], grad_fn=&lt;SumBackward1&gt;)
<pre class="line-numbers language-none"><code class="language-none">
  

### 把index转化为对应的boolean array作为mask：直接索引即可

&#96;&#96;&#96;python
idx &#x3D; torch.tensor([3, 7, 2])
bool_mask &#x3D; torch.zeros(10)
bool_mask[idx] &#x3D; 1
bool_mask &#x3D; bool_mask.bool()
bool_mask<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-none"><code class="language-none">tensor([False, False,  True,  True, False, False, False,  True, False, False])<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

</code></pre>
</li>
</ul>
<h2 id="获取tensor的device-GPU-CPU均可）-tensor-device"><a href="#获取tensor的device-GPU-CPU均可）-tensor-device" class="headerlink" title="获取tensor的device(GPU, CPU均可）:tensor.device"></a>获取tensor的device(GPU, CPU均可）:<code>tensor.device</code></h2><p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/which-device-is-model-tensor-stored-on/4908/14">Which device is model / tensor stored on?</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">cuda_check <span class="token operator">=</span> my_tensor<span class="token punctuation">.</span>is_cuda
<span class="token keyword">if</span> cuda_check<span class="token punctuation">:</span>
    get_cuda_device <span class="token operator">=</span> my_tensor<span class="token punctuation">.</span>get_device<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>


<ul>
<li>如果是gpu的tensor，则会输出 <strong>对应的CUDA编号</strong>（如在CUDA:0的tensor则会返回0）</li>
<li>如果是cpu的tensor，<strong>则默认返回-1</strong></li>
</ul>
<h2 id="torch-device的用法"><a href="#torch-device的用法" class="headerlink" title="torch.device的用法"></a><code>torch.device</code>的用法</h2><ul>
<li><p>直接<code>torch.device(&#39;cuda&#39;)</code>等价于得到一个 <strong>当前cuda device的objects（有多个gpu时默认是第0个）</strong></p>
<blockquote>
<p> a <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><code>torch.Tensor</code></a> constructed with device <code>&#39;cuda&#39;</code> is equivalent to <code>&#39;cuda:X&#39;</code> where X is the result of <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/cuda.html#torch.cuda.current_device"><code>torch.cuda.current_device()</code></a></p>
</blockquote>
</li>
<li><p>tensor的device可通过<code> Tensor.device</code>获得</p>
</li>
<li><p>对于cuda来说，以下两种写法都是等价的：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>推荐用上面的写法，更清晰。</p>
</li>
<li><p>注意，**<code>torch.cuda.device</code>是一个context -manager， 即需要通过<code>with torch.cuda.device():</code>**来使用（让context内的tensor都在这个cuda上）</p>
</li>
</ul>
<h2 id="pytorch-scatter报错：scatter-cpu-undefined-symbol-ZN2at6detail20DynamicCUDAInterface10set-deviceE"><a href="#pytorch-scatter报错：scatter-cpu-undefined-symbol-ZN2at6detail20DynamicCUDAInterface10set-deviceE" class="headerlink" title="pytorch_scatter报错：scatter_cpu undefined symbol: _ZN2at6detail20DynamicCUDAInterface10set_deviceE"></a>pytorch_scatter报错：scatter_cpu undefined symbol: _ZN2at6detail20DynamicCUDAInterface10set_deviceE</h2><p>原因是因为pytorch升级了，需要重新编译。但是当前还未支持1.3.0，所以需要重新为1.2.0</p>
<h2 id="直接计算KL-divergence-torch-nn-KLDivLoss或者其对应的函数形式"><a href="#直接计算KL-divergence-torch-nn-KLDivLoss或者其对应的函数形式" class="headerlink" title="直接计算KL_divergence:torch.nn.KLDivLoss或者其对应的函数形式"></a>直接计算KL_divergence:<code>torch.nn.KLDivLoss</code>或者其对应的函数形式</h2><ul>
<li>注意具体计算的时候，输入的顺序：<strong>函数中的 p q 位置相反，即如果要D(p||q)，则应该写成kl_div（q.log（），p）的形式，q要用F.log或者F.log_softmax</strong>。具体参考<a target="_blank" rel="noopener" href="https://blog.csdn.net/wwyy2018/article/details/101599862">pytorch中的kl divergence计算问题</a></li>
</ul>
<h2 id="pytorch支持直接通过index逐个赋值"><a href="#pytorch支持直接通过index逐个赋值" class="headerlink" title="pytorch支持直接通过index逐个赋值"></a>pytorch支持直接通过index逐个赋值</h2><ul>
<li><p>即当 <strong>index是一个list的时候，可以通过两边都用同样的list来实现逐个赋值</strong>，这个list<strong>既可以是离散的list，也可以是<code>:</code>组成的slicing</strong></p>
</li>
<li><p>例子：离散的list，最后的结果的确tt的1，3，4行都改变了：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">eval_ema_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span>
Out<span class="token punctuation">[</span><span class="token number">116</span><span class="token punctuation">]</span><span class="token punctuation">:</span> 
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tt <span class="token operator">=</span> train_dataset<span class="token punctuation">.</span>labels<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
tt
Out<span class="token punctuation">[</span><span class="token number">118</span><span class="token punctuation">]</span><span class="token punctuation">:</span> 
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tt<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">=</span>eval_ema_pred<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
tt
Out<span class="token punctuation">[</span><span class="token number">122</span><span class="token punctuation">]</span><span class="token punctuation">:</span> 
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

</li>
<li><p>例子：<code>:</code>组成的slicing，一样可以，<code>eval_ema_pred_2</code>的最后三行的确改变值了：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">eval_ema_pred_2 <span class="token operator">=</span> eval_ema_pred<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
eval_ema_pred_2<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
Out<span class="token punctuation">[</span><span class="token number">126</span><span class="token punctuation">]</span><span class="token punctuation">:</span> 
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
eval_ema_pred_2<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> tt<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
eval_ema_pred_2<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
Out<span class="token punctuation">[</span><span class="token number">129</span><span class="token punctuation">]</span><span class="token punctuation">:</span> 
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


</li>
</ul>
<h2 id="learning-rate-paramter-调整"><a href="#learning-rate-paramter-调整" class="headerlink" title="learning rate paramter 调整"></a>learning rate paramter 调整</h2><p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/adaptive-learning-rate/320">Adaptive learning rate</a></p>
<p>如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">adjust_learning_rate</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""</span>
    lr <span class="token operator">=</span> args<span class="token punctuation">.</span>lr <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">0.1</span> <span class="token operator">**</span> <span class="token punctuation">(</span>epoch <span class="token operator">//</span> <span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>
        param_group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="Reset-model"><a href="#Reset-model" class="headerlink" title="Reset model"></a>Reset model</h2><h3 id="手动初始化所有parameter"><a href="#手动初始化所有parameter" class="headerlink" title="手动初始化所有parameter"></a>手动初始化所有parameter</h3><p>可以参考这个：</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/cross-validation-model-reset/21176/3">Reset model</a></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">weights_init</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">)</span><span class="token punctuation">:</span>
        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>calculate_gain<span class="token punctuation">(</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        m<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">elif</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>

model<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>weights_init<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="直接重建"><a href="#直接重建" class="headerlink" title="直接重建"></a>直接重建</h3><p><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/53353203">Del Model</a></p>
<ul>
<li>注意：<strong>关键在于要保证原来的model的所有reference都被del了</strong>，这样model的那个空间才会被删除，因此下面两种做法各代表和会自动释放与不会自动释放</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># model won't be deleted by python since model still refer to resourece on gpu</span>
model_cpu <span class="token operator">=</span> model<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># This way original resource on gpu will be released by python</span>
model <span class="token operator">=</span> model<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h2 id="torch-tensor-to没有inplace-version"><a href="#torch-tensor-to没有inplace-version" class="headerlink" title="torch.tensor.to没有inplace version"></a><code>torch.tensor.to</code>没有inplace version</h2><p>因此每一次需要用原来的变量保存</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">data <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>


<p>使用DataParallel后再访问attribute：需要用<code>module.module.attribute</code></p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/how-to-reach-model-attributes-wrapped-by-nn-dataparallel/1373/5">how-to-reach-model-attributes-wrapped-by-nn-dataparallel</a></p>
<pre class="line-numbers language-none"><code class="language-none">Allnet.module.test_forward()<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="torch-nn-Module-state-dict-的一些特性"><a href="#torch-nn-Module-state-dict-的一些特性" class="headerlink" title="torch.nn.Module.state_dict()的一些特性"></a>torch.nn.Module.state_dict()的一些特性</h2><p>根据官方说明：</p>
<blockquote>
<p>Returns a dictionary containing a whole state of the module.<br>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</p>
</blockquote>
<ul>
<li>返回的是一个Orderdict，包含了所有层的参数，和一些persistent buffer，如平均值等。</li>
<li>所返回的Orderdict都是默认<strong>没有梯度的</strong>  <pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'conv1.weight'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad
Out<span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h3 id="clone-model的方法"><a href="#clone-model的方法" class="headerlink" title="clone model的方法"></a>clone model的方法</h3>基于上述性质，可以直接把state_dict()保存到一个临时变量中，然后直接load_dict()</li>
</ul>
<h2 id="pytorch获取tensor占用大小"><a href="#pytorch获取tensor占用大小" class="headerlink" title="pytorch获取tensor占用大小"></a>pytorch获取tensor占用大小</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">sys<span class="token punctuation">.</span>getsizeof<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="stack-和cat-的使用场景"><a href="#stack-和cat-的使用场景" class="headerlink" title="stack()和cat()的使用场景"></a>stack()和cat()的使用场景</h2><ul>
<li>主要区别就是，stack会增加一个维度，而cat不会。二者都要求除了被拼接的那个维度，<strong>其余维度都需要相同</strong></li>
<li>因此，stack对于原本element维度为0的list，拼起来之后自然就变成了1d的tensor；而对于想把element为1d的list拼起来，则应该用cat<h3 id="list-of-single-item-tensor-stack"><a href="#list-of-single-item-tensor-stack" class="headerlink" title="list of single item tensor: stack"></a>list of single item tensor: stack</h3></li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token operator">>></span><span class="token operator">></span>Traceback <span class="token punctuation">(</span>most recent call last<span class="token punctuation">)</span><span class="token punctuation">:</span>
  File <span class="token string">"/home/weitaotang/.conda/envs/tfpt_py3_backup/lib/python3.6/site-packages/IPython/core/interactiveshell.py"</span><span class="token punctuation">,</span> line <span class="token number">3326</span><span class="token punctuation">,</span> <span class="token keyword">in</span> run_code
    <span class="token keyword">exec</span><span class="token punctuation">(</span>code_obj<span class="token punctuation">,</span> self<span class="token punctuation">.</span>user_global_ns<span class="token punctuation">,</span> self<span class="token punctuation">.</span>user_ns<span class="token punctuation">)</span>
  File <span class="token string">"&lt;ipython-input-14-6589a900f85f>"</span><span class="token punctuation">,</span> line <span class="token number">1</span><span class="token punctuation">,</span> <span class="token keyword">in</span> <span class="token operator">&lt;</span>module<span class="token operator">></span>
    torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
RuntimeError<span class="token punctuation">:</span> zero<span class="token operator">-</span>dimensional tensor <span class="token punctuation">(</span>at position <span class="token number">0</span><span class="token punctuation">)</span> cannot be concatenated
<span class="token operator">>></span><span class="token operator">></span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="list-of-1d-tensor-cat-如果希望获得1d的tensor"><a href="#list-of-1d-tensor-cat-如果希望获得1d的tensor" class="headerlink" title="list of 1d tensor: cat(如果希望获得1d的tensor)"></a>list of 1d tensor: cat(如果希望获得1d的tensor)</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">:</span> 
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">]</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>




<h2 id="torch-no-grad-和-model-eval-怎么用？"><a href="#torch-no-grad-和-model-eval-怎么用？" class="headerlink" title="torch.no_grad()和 model.eval()怎么用？"></a><code>torch.no_grad()</code>和 <code>model.eval()</code>怎么用？</h2><p>参考</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/55627781">Evaluating pytorch models: <code>with torch.no_grad</code> vs <code>model.eval()</code></a></p>
<p>提及是否要重新进行<code>torch.set_grad_enabled(True)</code>的答案<a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/37"><code>model_eval()</code> vs     <code>with torch.no_grad()</code></a></p>
<ul>
<li><p>最好两个一起用，因为二者的作用不同：</p>
<pre><code>1. `torch.no_grad()`是切断autograd，保证没有梯度，并且能够让inference时所占用的内存更少（因为不需要保存gradients）
 2. `model.eval()`则是让某些层的行为发生改变，比如dropou层在开启了`eval()`后就不再作用，batchnorm则是会在整个input上
</code></pre>
</li>
<li><p>在<code>model.eval()</code>完了之后并不用设置<code>set_grad_enabled</code>，参考第二个回答</p>
</li>
</ul>
<p>TensorboardX对于Multiline的prettytable的处理：</p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/46505527">TensorFlow: tf.summary.text and linebreaks</a></p>
<ul>
<li><p>因此需要把空格和<code>\n</code>替换 掉，如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">table<span class="token punctuation">.</span>get_string<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">,</span> <span class="token string">'&amp;nbsp;'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span> <span class="token string">'  \n'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<h2 id="pytorch的GPU-memory管理"><a href="#pytorch的GPU-memory管理" class="headerlink" title="pytorch的GPU memory管理"></a>pytorch的GPU memory管理</h2></li>
</ul>
<p>pytorch的GPU管理类似python的，当没有引用指向那部分内存的时候，就能够通过<code>torch.cuda.empty_cache()</code>释放掉，<strong>因此要特别注意forward等函数中的中间变量的使用</strong></p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/faq.html">My model reports “cuda runtime error(2): out of memory”</a> 中提到了关于loss的处理，千万不要accumulate，否则很麻烦.</p>
<p>  错误的做法：<br>  loss中包含了grad，因此不断append会越来越大</p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python">loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
total_loss <span class="token operator">+=</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<p>  正确的做法：用 <code>item()</code></p>
  <pre class="line-numbers language-python" data-language="python"><code class="language-python">loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> targets<span class="token punctuation">)</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
total_loss <span class="token operator">+=</span> loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li>
<li><p>对于无需训练的部分（inference那部分），建议直接用 <code>torch.no_grad()</code></p>
</li>
</ul>
<h3 id="查看显存占用情况"><a href="#查看显存占用情况" class="headerlink" title="查看显存占用情况"></a>查看显存占用情况</h3><p>参考：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_28660035/article/details/80688427">pytorch 减小显存消耗，优化显存使用，避免out of memory</a> 里面罗列了很多。</li>
<li><a target="_blank" rel="noopener" href="https://oldpan.me/archives/how-to-calculate-gpu-memory">浅谈深度学习:如何计算模型以及中间变量的显存占用大小</a> 这位大神有写一个具体的package来显示各个tensor占用的内存。</li>
</ol>
<h2 id="复制模型的结构，如Sequential"><a href="#复制模型的结构，如Sequential" class="headerlink" title="复制模型的结构，如Sequential"></a>复制模型的结构，如Sequential</h2><p>直接把原来的那个Sequential的module抽出来就好了。默认<code>_modules</code>是OrderedDict，因此再用<code>values</code>就可转化为list, <code>nn.Sequential</code>等价于一个新的Constructor</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">s1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
s2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>s1<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<h2 id="解决RuntimeError-CUDA-error-initialization-error的问题"><a href="#解决RuntimeError-CUDA-error-initialization-error的问题" class="headerlink" title="解决RuntimeError: CUDA error: initialization error的问题"></a>解决RuntimeError: CUDA error: initialization error的问题</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yyhaohaoxuexi/article/details/90718501">RuntimeError: CUDA error: initialization error</a></p>
<p>避免这个问题要注意两点：</p>
<ol>
<li>当使用<code>num_workers</code>这个参数时，要注意成为Dataset的数据必须在CPU上，不能在GPU上。</li>
<li>Dataset的数据不能有梯度，可以通过<code>detach()</code>来实现。</li>
</ol>
<p>综上，当出现这样的问题的时候，可以尝试：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>

<h2 id="MSELoss初探"><a href="#MSELoss初探" class="headerlink" title="MSELoss初探"></a>MSELoss初探</h2><p>和其他loss类似，<code>reduction</code>选项有三个，分别是<code>sum</code>，<code>mean</code>，<code>none</code>。默认是mean。三者的区别如下：</p>
<ol>
<li>none；求完平方和之后不动，输出的维度和两个输入都相同</li>
<li>sum: 求完平方和之后再加起来</li>
<li>mean：在sum的基础上再除以len（如果是2d的输入，则除以$n \times m$</li>
</ol>
<p>测试的输出结果如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">In <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">:</span> a1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">:</span> a1
Out<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.5009</span><span class="token punctuation">,</span>  <span class="token number">1.2292</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1333</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.2095</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.6178</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.9349</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.8915</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4830</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.0462</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0175</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3017</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5426</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">:</span> a2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">:</span> a2
Out<span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.1398</span><span class="token punctuation">,</span>  <span class="token number">0.4038</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1434</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0798</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.1173</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3946</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4661</span><span class="token punctuation">,</span>  <span class="token number">0.2909</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.5324</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6149</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.4386</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.7462</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn

In <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">:</span> mse <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">:</span> mse<span class="token punctuation">(</span>a1<span class="token punctuation">,</span> a2<span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.6919</span><span class="token punctuation">,</span> <span class="token number">0.6814</span><span class="token punctuation">,</span> <span class="token number">1.0204</span><span class="token punctuation">,</span> <span class="token number">0.7575</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.2505</span><span class="token punctuation">,</span> <span class="token number">0.2920</span><span class="token punctuation">,</span> <span class="token number">2.0318</span><span class="token punctuation">,</span> <span class="token number">3.1468</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.2364</span><span class="token punctuation">,</span> <span class="token number">0.1620</span><span class="token punctuation">,</span> <span class="token number">1.2925</span><span class="token punctuation">,</span> <span class="token number">1.4487</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">:</span> mse <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'sum'</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">:</span> mse<span class="token punctuation">(</span>a1<span class="token punctuation">,</span> a2<span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token number">14.0119</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">13</span><span class="token punctuation">]</span><span class="token punctuation">:</span> mse <span class="token operator">=</span> nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'mean'</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">:</span> mse<span class="token punctuation">(</span>a1<span class="token punctuation">,</span> a2<span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token number">1.1677</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">14.0119</span> <span class="token operator">/</span> <span class="token number">12</span>
Out<span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">:</span> <span class="token number">1.1676583333333335</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>但是如果只是单纯算距离的话，就不应该用这个函数，这个本质上还是loss，而应该用下面详述的<code>pdist</code></p>
<h2 id="算p-norm用的函数：torch-nn-pdist"><a href="#算p-norm用的函数：torch-nn-pdist" class="headerlink" title="算p-norm用的函数：torch.nn.pdist()"></a>算p-norm用的函数：<code>torch.nn.pdist()</code></h2><p>实际就是把dist matrix的upper triangular part算了出来，因此当输入是$N \times M$的时候，输出会是$1/2 \times N \times N$，具体例子如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">In <span class="token punctuation">[</span><span class="token number">26</span><span class="token punctuation">]</span><span class="token punctuation">:</span> b <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">]</span><span class="token punctuation">:</span> b
Out<span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">28</span><span class="token punctuation">]</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>pdist<span class="token punctuation">(</span>b<span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">28</span><span class="token punctuation">]</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.4142</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="获得dist-matrix的正确操作：torch-triu-转置即可"><a href="#获得dist-matrix的正确操作：torch-triu-转置即可" class="headerlink" title="获得dist matrix的正确操作：torch.triu + 转置即可"></a>获得dist matrix的正确操作：<code>torch.triu</code> + 转置即可</h2><p><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/58806626">transform the upper/lower triangular part of a symmetric matrix (2D array) into a 1D array and return it to the 2D format</a>，按照这里面所说的，在numpy中直接<code>triu_indices/tril_indices</code>结合转置的操作即可。</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/whats-the-most-efficient-way-to-convert-a-vector-into-a-triangular-matrix/27301">What’s the most efficient way to convert a vector into a triangular matrix?</a> 这里面有给很简单的例子</p>
<p>因此在pytorch中，具体的操作应该是如下：先通过<code>triu_indices</code>获得upper triangular part的indices，逐个赋值，然后转置，最后相加即可</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">In <span class="token punctuation">[</span><span class="token number">26</span><span class="token punctuation">]</span><span class="token punctuation">:</span> b <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">]</span><span class="token punctuation">:</span> b
Out<span class="token punctuation">[</span><span class="token number">27</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">28</span><span class="token punctuation">]</span><span class="token punctuation">:</span> F<span class="token punctuation">.</span>pdist<span class="token punctuation">(</span>b<span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">28</span><span class="token punctuation">]</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.4142</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">29</span><span class="token punctuation">]</span><span class="token punctuation">:</span> dist <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>b<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">:</span> dist
Out<span class="token punctuation">[</span><span class="token number">30</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
In <span class="token punctuation">[</span><span class="token number">38</span><span class="token punctuation">]</span><span class="token punctuation">:</span> dist<span class="token punctuation">[</span>triu_indices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> triu_indices<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">=</span>F<span class="token punctuation">.</span>pdist<span class="token punctuation">(</span>b<span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">39</span><span class="token punctuation">]</span><span class="token punctuation">:</span> dist
Out<span class="token punctuation">[</span><span class="token number">39</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">1.4142</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

In <span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">]</span><span class="token punctuation">:</span> dist<span class="token punctuation">.</span>T
Out<span class="token punctuation">[</span><span class="token number">40</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.4142</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">3.6056</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
In <span class="token punctuation">[</span><span class="token number">41</span><span class="token punctuation">]</span><span class="token punctuation">:</span> dist <span class="token operator">=</span> dist <span class="token operator">+</span> dist<span class="token punctuation">.</span>T

In <span class="token punctuation">[</span><span class="token number">42</span><span class="token punctuation">]</span><span class="token punctuation">:</span> dist
Out<span class="token punctuation">[</span><span class="token number">42</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">1.4142</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.4142</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">3.6056</span><span class="token punctuation">,</span> <span class="token number">3.6056</span><span class="token punctuation">,</span> <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>PS：如果想把上述的L2 norm距离（即euclidean distance）转化为similarity，可以考虑如下的操作：<br>（把其中操作换成inplace的也可行）</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">In <span class="token punctuation">[</span><span class="token number">68</span><span class="token punctuation">]</span><span class="token punctuation">:</span> dist_<span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
Out<span class="token punctuation">[</span><span class="token number">68</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token number">0.2500</span><span class="token punctuation">,</span> <span class="token number">0.1161</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.2500</span><span class="token punctuation">,</span> <span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token number">0.0979</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.1161</span><span class="token punctuation">,</span> <span class="token number">0.0979</span><span class="token punctuation">,</span> <span class="token number">1.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>




<h2 id="model-eval-和-model-train-对同一个batch输出差异巨大的原因"><a href="#model-eval-和-model-train-对同一个batch输出差异巨大的原因" class="headerlink" title="model.eval() 和 model.train() 对同一个batch输出差异巨大的原因"></a>model.eval() 和 model.train() 对同一个batch输出差异巨大的原因</h2><p>最主要就是BatchNorm出问题了。根据官方文档</p>
<blockquote>
<p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which <strong>are then used for normalization</strong> during evaluation. </p>
</blockquote>
<p>因此如果training的时候统计出来的mean和variance与test的时候差异巨大，就会导致最后输出的loss或者是prediction差异巨大。</p>
<p><strong>因此这个问题的出现通常是batch size设置太大的缘故</strong>，为了保证精度，应该：</p>
<ul>
<li>把batch size设置小一些，这样才能让这个统计更加准确，</li>
<li>不要逐个epoch都test一次，在训练了一定次数之后再开始test</li>
</ul>
<p>参考如下</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/39">Ptrick’s answer</a></li>
</ol>
<blockquote>
<p>During training the current batch stats will be used to compute the output, so that the model might converge.<br>However, during evaluation the batchnorm layer tries to normalize both inputs with skewed running estimates, which yields the high loss values.<br>Usually we assume that all inputs are from the same domain and thus have approx. the same statistics</p>
</blockquote>
<ol start="2">
<li><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561/3">Unstable training leads to incorrect estimate</a></li>
</ol>
<blockquote>
<p>it is possible that your training in general is unstable, so BatchNorm’s running_mean and running_var dont represent true batch statistics.</p>
</blockquote>
<pre><code>即batch size太大，估计出来的variance和mean就不太准了。（因为train set和test set shuffle的方式通常不一样）
</code></pre>
<h2 id="手动更新parameters-Autograd使用"><a href="#手动更新parameters-Autograd使用" class="headerlink" title="手动更新parameters/Autograd使用"></a>手动更新parameters/Autograd使用</h2><p>参考：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/nonetype-object-has-no-attribute-zero/61013">‘NoneType’ object has no attribute ‘zero_’</a> 说的很清楚</li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#ea0d">Autograd</a>，一个pytorch的例子，也比较清楚</li>
<li><a target="_blank" rel="noopener" href="https://github.com/jcjohnson/pytorch-examples#pytorch-autograd">PyTorch: Autograd</a>很清晰的例子，推荐参考！</li>
</ol>
<h3 id="preliminary"><a href="#preliminary" class="headerlink" title="preliminary"></a>preliminary</h3><ul>
<li>创建一个新的<code>Tensor</code>后，默认是<ul>
<li><code>require_grad=True</code></li>
<li><code>grad</code>为<code>None</code>，说明grad为全0。因此如果一开始直接<code>grad.zero()_</code>是会报错的</li>
</ul>
</li>
</ul>
<h3 id="容易遇到的问题"><a href="#容易遇到的问题" class="headerlink" title="容易遇到的问题"></a>容易遇到的问题</h3><ul>
<li>不是<code>inplace</code>操作：形如<code>weight = weight - weight.grad*lr</code>会导致创建一个新的变量，而新的变量会丢失之前变量的信息，包括<code>grad</code>。导致后面直接<code>.grad.zero_()</code>错误</li>
<li>没有把手动update这个操作放到<code>torch.no_grad()</code>中：导致不必要的<code>grad</code>计算以及错误的结果</li>
</ul>
<h3 id="正确方法"><a href="#正确方法" class="headerlink" title="正确方法"></a>正确方法</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    predict <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>feature<span class="token punctuation">,</span> weight<span class="token punctuation">)</span> <span class="token operator">+</span> bias<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>predict <span class="token operator">-</span> label<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># Disable the autograd</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Inplace changes</span>
        weight<span class="token punctuation">.</span>sub_<span class="token punctuation">(</span>weight<span class="token punctuation">.</span>grad<span class="token operator">*</span>lr<span class="token punctuation">)</span>
        bias<span class="token punctuation">.</span>sub_<span class="token punctuation">(</span>bias<span class="token punctuation">.</span>grad<span class="token operator">*</span>lr<span class="token punctuation">)</span> <span class="token comment"># A .grad is missing in your code here I think ;)</span>
        <span class="token comment"># Do the reset in no grad mode as well in case you do second order</span>
        <span class="token comment"># derivatives later (meaning that weight.grad will requires_grad)</span>
        weight<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
        bias<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>




<h1 id="stuck-in-data-loading：一直卡死在载入数据上"><a href="#stuck-in-data-loading：一直卡死在载入数据上" class="headerlink" title="stuck in data loading：一直卡死在载入数据上"></a>stuck in data loading：一直卡死在载入数据上</h1><p>参考 <a target="_blank" rel="noopener" href="https://youtrack.jetbrains.com/issue/PY-39489">Debugger freezes stepping forward when using pytorch with workers (multiprocessing)</a></p>
<p>原因：可能是<code>num_workers</code>的问题，<strong>设置为0！就解决了！</strong></p>
<img src="https://raw.githubusercontent.com/khalitt/IMG_REPO/master/20210114233215.png" align="center/">



<h1 id="clone-detach-torch-tensor-的比较"><a href="#clone-detach-torch-tensor-的比较" class="headerlink" title="clone  detach  torch.tensor 的比较"></a><code>clone</code>  <code>detach </code> <code>torch.tensor</code> 的比较</h1><p>参考：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/winycg/article/details/100813519">Pytorch张量（Tensor）复制</a>，把<code>clone</code>和<code>detach</code>的几种实验关系都说清楚了</li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/62485994">Why Tensor.clone().detach() is recommended when copying a tensor?</a>，主要提及了默认的<code>torch.tensor</code>在创建新变量的时候的默认属性</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor">torch.tensor</a>：官方文档说明</li>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/a/59631230">Pytorch preferred way to copy a tensor</a>这个回答也说的很清楚</li>
<li><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/difference-between-detach-clone-and-clone-detach/34173">Difference between detach().clone() and clone().detach()</a>，本质二者没区别</li>
</ol>
<h2 id="创建一个新的leaf-node"><a href="#创建一个新的leaf-node" class="headerlink" title="创建一个新的leaf node"></a>创建一个新的leaf node</h2><p>来自<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor">torch.tensor</a>：</p>
<blockquote>
<p>When data is a tensor x, <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"><code>torch.tensor()</code></a> reads out ‘the data’ from whatever it is passed, and <strong>constructs a leaf variable.</strong> Therefore <code>torch.tensor(x)</code> is equivalent to <code>x.clone().detach()</code> and <code>torch.tensor(x, requires_grad=True)</code> is equivalent to <code>x.clone().detach().requires_grad_(True)</code>. <strong>The equivalents using <code>clone()</code> and <code>detach()</code> are recommended.</strong></p>
</blockquote>
<p>即最快的方法是：直接<code>torch.tensor(x)</code>，而这等价于：</p>
<ol>
<li><code>x.clone().detach()</code>获取到一个内部的数据，且从计算图切断</li>
<li>用上述获得的data（<code>item()</code>）创建一个新的<code>torch.tensor</code></li>
</ol>
<p>然而直接用上述的方法是不建议的，会有warning：</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">def new_tensor<span class="token punctuation">(</span>x, <span class="token assign-left variable">requires_grad</span><span class="token operator">=</span>True<span class="token punctuation">)</span>:
    <span class="token builtin class-name">return</span> torch.tensor<span class="token punctuation">(</span>x, <span class="token assign-left variable">requires_grad</span><span class="token operator">=</span>requires_grad, <span class="token assign-left variable">device</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">)</span> <span class="token punctuation">\</span>
        <span class="token keyword">if</span> torch.cuda.is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch.tensor<span class="token punctuation">(</span>x, <span class="token assign-left variable">requires_grad</span><span class="token operator">=</span>requires_grad, <span class="token assign-left variable">device</span><span class="token operator">=</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<img src="https://raw.githubusercontent.com/khalitt/IMG_REPO/master/20210116162319.png" align="center/">



<p>参考 <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/it-is-recommended-to-use-source-tensor-clone-detach-or-sourcetensor-clone-detach-requires-grad-true/101218">It is recommended to use source Tensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True)</a>的回答</p>
<blockquote>
<p><strong>The warning points to wrapping a tensor in torch.tensor, which is not recommended</strong>. Instead of torch.tensor(outputs) <strong>use outputs.clone().detach() or the same with .requires_grad_(True)</strong>, if necessary.</p>
</blockquote>
<p>据官方文档，最佳的还是<code>x.clone().detach().requires_grad_(True)</code>或者<code>x.clone().detach().requires_grad_(False)</code></p>
<p><strong>特别注意，如果是想从cpu创建到gpu上，需要保证先迁移到GPU上，然后再开启<code>requires_grad_()</code>**（参考 <a target="_blank" rel="noopener" href="https://blog.csdn.net/nkhgl/article/details/100047276">【pytorch: can’t optimize a non-leaf Tensor】</a>。否则</strong>会因为先开始记录grad，再迁移，导致GPU上的tensor多了个<code>copyfnbackward</code>，即在GPU上的不再是叶子节点**：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 错误做法，会导致 ValueError: can‘t optimize a non-leaf Tensor</span>
new_x <span class="token operator">=</span> x<span class="token punctuation">.</span>clone<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 正确做法</span>
new_x <span class="token operator">=</span> x<span class="token punctuation">.</span>clone<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>


<h1 id="nn-Parameters"><a href="#nn-Parameters" class="headerlink" title="nn.Parameters"></a>nn.Parameters</h1><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter">torch.nn.parameter</a></p>
<blockquote>
<p>A kind of Tensor that is to be considered a module parameter.</p>
</blockquote>
<p>实际是<code>torch.Tensor</code>的子类，可以通过<code>.data</code>获取到对应的tensors。更多说明可以看上面的文档</p>
<h1 id="把多个N-dim的tensor拼成N-1-dim的tensor"><a href="#把多个N-dim的tensor拼成N-1-dim的tensor" class="headerlink" title="把多个N dim的tensor拼成N+1 dim的tensor"></a>把多个N dim的tensor拼成N+1 dim的tensor</h1><p>直接用<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.stack.html#torch-stack"><code>torch.stack</code></a>就可以了！！</p>
<blockquote>
<p>Concatenates a sequence of tensors along a new dimension.</p>
<p>All tensors need to be of the same size.</p>
</blockquote>
<p>还可以指定对出来的那个维度放在哪！(<code>dim</code>)</p>
<p>一个典型的应用就是 <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/element-wise-sum-of-batched-tensors/100988"><strong>element-wise sum of a batched tensors</strong></a></p>
<img src="https://raw.githubusercontent.com/khalitt/IMG_REPO/master/20210131112839.png" align="center/">



<h1 id="torch-as-tensor-和torch-from-numpy-的区别"><a href="#torch-as-tensor-和torch-from-numpy-的区别" class="headerlink" title="torch.as_tensor() 和torch.from_numpy()的区别"></a><code>torch.as_tensor()</code> 和<code>torch.from_numpy()</code>的区别</h1><p>参考 <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/from-numpy-vs-as-tensor/79932">From_numpy vs as_tensor</a></p>
<blockquote>
<p>Yes, <strong>both approaches share the underlying memory.</strong><br><code>torch.as_tensor</code> accepts a bit more that <code>torch.from_numpy</code>, such as <code>list</code> objects, and might thus have a slightly higher overhead for these checks.</p>
</blockquote>
<p>所以对于<code>numpy.array()</code>来说，几乎没差别</p>
<h1 id="torch-nn-ModuleList-导致的UserWarning-Setting-attributes-on-ParameterList-is-not-supported"><a href="#torch-nn-ModuleList-导致的UserWarning-Setting-attributes-on-ParameterList-is-not-supported" class="headerlink" title="torch.nn.ModuleList()导致的UserWarning: Setting attributes on ParameterList is not supported. "></a><code>torch.nn.ModuleList()</code>导致的<code>UserWarning: Setting attributes on ParameterList is not supported. </code></h1><p><img src="C:/Users/walter/AppData/Roaming/Typora/typora-user-images/image-20210206175230565.png" alt="image-20210206175230565"></p>
<p>把实现方式从</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">self<span class="token punctuation">.</span>factors <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterList<span class="token punctuation">(</span>
    <span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>rank<span class="token punctuation">,</span> <span class="token number">10</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span>
     <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>mds<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>


<p>变为</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> num_md <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>num_mds<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>factors<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>self<span class="token punctuation">.</span>rank<span class="token punctuation">,</span> <span class="token number">10</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token builtin">setattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string-interpolation"><span class="token string">f"factor_</span><span class="token interpolation"><span class="token punctuation">&#123;</span>num_md<span class="token punctuation">&#125;</span></span><span class="token string">"</span></span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>factors<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>


<p>这样打印<code>model.name_parameters()</code>时，就从</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">&#123;</span><span class="token string">'fusion_weights'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fusion_bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.0.0.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.0.0.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.0.2.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.0.2.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.0.4.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.0.4.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.0.6.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.0.6.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.1.0.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.1.0.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.1.2.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.1.2.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.1.4.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.1.4.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.1.6.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'all_fcs.1.6.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'factors.0'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'factors.1'</span><span class="token builtin class-name">:</span> None<span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>变为</p>
<pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token punctuation">&#123;</span><span class="token string">'factor_0'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'factor_1'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fusion_weights'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fusion_bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_0.0.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_0.0.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_0.2.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_0.2.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_0.4.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_0.4.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_0.6.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_0.6.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_1.0.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_1.0.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_1.2.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_1.2.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_1.4.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_1.4.bias'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_1.6.weight'</span><span class="token builtin class-name">:</span> None,
 <span class="token string">'fc_1.6.bias'</span><span class="token builtin class-name">:</span> None<span class="token punctuation">&#125;</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>


<p><img src="C:/Users/walter/AppData/Roaming/Typora/typora-user-images/image-20210207105445811.png" alt="image-20210207105445811"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Walter</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://khalitt.github.io/2019/09/10/pytorch-basic/">https://khalitt.github.io/2019/09/10/pytorch-basic/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a><a class="post-meta__tags" href="/tags/basic/">basic</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2019/09/10/python-basic/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">python-basic</div></div></a></div><div class="next-post pull-right"><a href="/2019/09/10/python-configparser/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">python config 读取与config parser的使用</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2019/12/04/mmdetection-code-explanation/" title="mmdetection源码解析"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-12-04</div><div class="title">mmdetection源码解析</div></div></a></div><div><a href="/2019/09/17/pytorch-dataloader-and-sampler/" title="pytorch中的Dataloader与Sampler(以及collate_fn)"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-09-17</div><div class="title">pytorch中的Dataloader与Sampler(以及collate_fn)</div></div></a></div><div><a href="/2021/01/08/pytorch-meta-learning/" title="pytorch-meta-learning"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-08</div><div class="title">pytorch-meta-learning</div></div></a></div><div><a href="/2021/02/04/pytorch-speed-up/" title="pytorch加速与降低显存"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-04</div><div class="title">pytorch加速与降低显存</div></div></a></div><div><a href="/2021/02/03/problems-record/pytorch-does-not-require-grad-and-does-not-have-a-grad_fn/" title="解决Pytorch RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-03</div><div class="title">解决Pytorch RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</div></div></a></div><div><a href="/2021/02/01/problems-record/pytorch-handling-nan/" title="pytorch nan处理"><img class="cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-02-01</div><div class="title">pytorch nan处理</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor%E8%BD%AC%E5%8C%96%E4%B8%BAlist"><span class="toc-number">1.</span> <span class="toc-text">tensor转化为list</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dilation%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">2.</span> <span class="toc-text">dilation的含义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%A2%AF%E5%BA%A6"><span class="toc-number">3.</span> <span class="toc-text">查看梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8hook"><span class="toc-number">3.1.</span> <span class="toc-text">用hook</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NLLLoss%E4%B8%8ECrossEntropyLoss"><span class="toc-number">4.</span> <span class="toc-text">NLLLoss与CrossEntropyLoss</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#NLLLoss"><span class="toc-number">4.1.</span> <span class="toc-text">NLLLoss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A7%E7%94%A8"><span class="toc-number">4.2.</span> <span class="toc-text">巧用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrossEntropyLoss"><span class="toc-number">4.3.</span> <span class="toc-text">CrossEntropyLoss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%98%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">乘法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-where-torch-Tensor-where"><span class="toc-number">5.1.</span> <span class="toc-text">torch.where() &#x2F; torch.Tensor.where()</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96idx-torch-nonzero"><span class="toc-number">6.</span> <span class="toc-text">获取idx: torch.nonzero</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8E%B7%E5%BE%97validation%EF%BC%88%E5%88%87%E5%88%86dataset"><span class="toc-number">7.</span> <span class="toc-text">获得validation（切分dataset)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#random-split"><span class="toc-number">7.1.</span> <span class="toc-text">random_split</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SubsetRandomSampler"><span class="toc-number">7.2.</span> <span class="toc-text">SubsetRandomSampler</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%B7%B1%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.3.</span> <span class="toc-text">自己实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision%E4%B8%AD%E7%9A%84transform"><span class="toc-number">8.</span> <span class="toc-text">torchvision中的transform</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataloader%E5%90%8C%E6%97%B6%E8%BE%93%E5%87%BAbatch-%E5%AF%B9%E5%BA%94%E7%9A%84index"><span class="toc-number">9.</span> <span class="toc-text">Dataloader同时输出batch 对应的index</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E6%9B%B4%E6%94%B9Dataset%E7%9A%84-getitem"><span class="toc-number">9.1.</span> <span class="toc-text">通过更改Dataset的__getitem__</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Infinite-Dataloader"><span class="toc-number">10.</span> <span class="toc-text">Infinite Dataloader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BD%93Dataset-getitem-%E8%BF%94%E5%9B%9E%E4%B8%80%E4%B8%AAdict%E6%97%B6"><span class="toc-number">11.</span> <span class="toc-text">当Dataset.__getitem__返回一个dict时</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax%E7%9A%84dim"><span class="toc-number">12.</span> <span class="toc-text">Softmax的dim</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96tensor%E7%9A%84device-GPU-CPU%E5%9D%87%E5%8F%AF%EF%BC%89-tensor-device"><span class="toc-number">13.</span> <span class="toc-text">获取tensor的device(GPU, CPU均可）:tensor.device</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-device%E7%9A%84%E7%94%A8%E6%B3%95"><span class="toc-number">14.</span> <span class="toc-text">torch.device的用法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch-scatter%E6%8A%A5%E9%94%99%EF%BC%9Ascatter-cpu-undefined-symbol-ZN2at6detail20DynamicCUDAInterface10set-deviceE"><span class="toc-number">15.</span> <span class="toc-text">pytorch_scatter报错：scatter_cpu undefined symbol: _ZN2at6detail20DynamicCUDAInterface10set_deviceE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E8%AE%A1%E7%AE%97KL-divergence-torch-nn-KLDivLoss%E6%88%96%E8%80%85%E5%85%B6%E5%AF%B9%E5%BA%94%E7%9A%84%E5%87%BD%E6%95%B0%E5%BD%A2%E5%BC%8F"><span class="toc-number">16.</span> <span class="toc-text">直接计算KL_divergence:torch.nn.KLDivLoss或者其对应的函数形式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E6%94%AF%E6%8C%81%E7%9B%B4%E6%8E%A5%E9%80%9A%E8%BF%87index%E9%80%90%E4%B8%AA%E8%B5%8B%E5%80%BC"><span class="toc-number">17.</span> <span class="toc-text">pytorch支持直接通过index逐个赋值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#learning-rate-paramter-%E8%B0%83%E6%95%B4"><span class="toc-number">18.</span> <span class="toc-text">learning rate paramter 调整</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reset-model"><span class="toc-number">19.</span> <span class="toc-text">Reset model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E5%88%9D%E5%A7%8B%E5%8C%96%E6%89%80%E6%9C%89parameter"><span class="toc-number">19.1.</span> <span class="toc-text">手动初始化所有parameter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B4%E6%8E%A5%E9%87%8D%E5%BB%BA"><span class="toc-number">19.2.</span> <span class="toc-text">直接重建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-tensor-to%E6%B2%A1%E6%9C%89inplace-version"><span class="toc-number">20.</span> <span class="toc-text">torch.tensor.to没有inplace version</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-nn-Module-state-dict-%E7%9A%84%E4%B8%80%E4%BA%9B%E7%89%B9%E6%80%A7"><span class="toc-number">21.</span> <span class="toc-text">torch.nn.Module.state_dict()的一些特性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#clone-model%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">21.1.</span> <span class="toc-text">clone model的方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E8%8E%B7%E5%8F%96tensor%E5%8D%A0%E7%94%A8%E5%A4%A7%E5%B0%8F"><span class="toc-number">22.</span> <span class="toc-text">pytorch获取tensor占用大小</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#stack-%E5%92%8Ccat-%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">23.</span> <span class="toc-text">stack()和cat()的使用场景</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#list-of-single-item-tensor-stack"><span class="toc-number">23.1.</span> <span class="toc-text">list of single item tensor: stack</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#list-of-1d-tensor-cat-%E5%A6%82%E6%9E%9C%E5%B8%8C%E6%9C%9B%E8%8E%B7%E5%BE%971d%E7%9A%84tensor"><span class="toc-number">23.2.</span> <span class="toc-text">list of 1d tensor: cat(如果希望获得1d的tensor)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-no-grad-%E5%92%8C-model-eval-%E6%80%8E%E4%B9%88%E7%94%A8%EF%BC%9F"><span class="toc-number">24.</span> <span class="toc-text">torch.no_grad()和 model.eval()怎么用？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E7%9A%84GPU-memory%E7%AE%A1%E7%90%86"><span class="toc-number">25.</span> <span class="toc-text">pytorch的GPU memory管理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E7%9C%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E6%83%85%E5%86%B5"><span class="toc-number">25.1.</span> <span class="toc-text">查看显存占用情况</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%8D%E5%88%B6%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%EF%BC%8C%E5%A6%82Sequential"><span class="toc-number">26.</span> <span class="toc-text">复制模型的结构，如Sequential</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3RuntimeError-CUDA-error-initialization-error%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">27.</span> <span class="toc-text">解决RuntimeError: CUDA error: initialization error的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#MSELoss%E5%88%9D%E6%8E%A2"><span class="toc-number">28.</span> <span class="toc-text">MSELoss初探</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97p-norm%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0%EF%BC%9Atorch-nn-pdist"><span class="toc-number">29.</span> <span class="toc-text">算p-norm用的函数：torch.nn.pdist()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8E%B7%E5%BE%97dist-matrix%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%93%8D%E4%BD%9C%EF%BC%9Atorch-triu-%E8%BD%AC%E7%BD%AE%E5%8D%B3%E5%8F%AF"><span class="toc-number">30.</span> <span class="toc-text">获得dist matrix的正确操作：torch.triu + 转置即可</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model-eval-%E5%92%8C-model-train-%E5%AF%B9%E5%90%8C%E4%B8%80%E4%B8%AAbatch%E8%BE%93%E5%87%BA%E5%B7%AE%E5%BC%82%E5%B7%A8%E5%A4%A7%E7%9A%84%E5%8E%9F%E5%9B%A0"><span class="toc-number">31.</span> <span class="toc-text">model.eval() 和 model.train() 对同一个batch输出差异巨大的原因</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%9B%B4%E6%96%B0parameters-Autograd%E4%BD%BF%E7%94%A8"><span class="toc-number">32.</span> <span class="toc-text">手动更新parameters&#x2F;Autograd使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#preliminary"><span class="toc-number">32.1.</span> <span class="toc-text">preliminary</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%B9%E6%98%93%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">32.2.</span> <span class="toc-text">容易遇到的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E7%A1%AE%E6%96%B9%E6%B3%95"><span class="toc-number">32.3.</span> <span class="toc-text">正确方法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#stuck-in-data-loading%EF%BC%9A%E4%B8%80%E7%9B%B4%E5%8D%A1%E6%AD%BB%E5%9C%A8%E8%BD%BD%E5%85%A5%E6%95%B0%E6%8D%AE%E4%B8%8A"><span class="toc-number"></span> <span class="toc-text">stuck in data loading：一直卡死在载入数据上</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#clone-detach-torch-tensor-%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-number"></span> <span class="toc-text">clone  detach  torch.tensor 的比较</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84leaf-node"><span class="toc-number">1.</span> <span class="toc-text">创建一个新的leaf node</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#nn-Parameters"><span class="toc-number"></span> <span class="toc-text">nn.Parameters</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8A%8A%E5%A4%9A%E4%B8%AAN-dim%E7%9A%84tensor%E6%8B%BC%E6%88%90N-1-dim%E7%9A%84tensor"><span class="toc-number"></span> <span class="toc-text">把多个N dim的tensor拼成N+1 dim的tensor</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torch-as-tensor-%E5%92%8Ctorch-from-numpy-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number"></span> <span class="toc-text">torch.as_tensor() 和torch.from_numpy()的区别</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#torch-nn-ModuleList-%E5%AF%BC%E8%87%B4%E7%9A%84UserWarning-Setting-attributes-on-ParameterList-is-not-supported"><span class="toc-number"></span> <span class="toc-text">torch.nn.ModuleList()导致的UserWarning: Setting attributes on ParameterList is not supported. </span></a></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Walter</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        addClass: [200,() => {
          document.querySelectorAll('mjx-container:not([display=\'true\']').forEach( node => {
            const target = node.parentNode
            if (!target.classList.contains('has-jax')) {
              target.classList.add('mathjax-overflow')
            }
          })
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>